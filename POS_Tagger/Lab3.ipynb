{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colx 525 Lab Assignment 3: POS Tagging\n",
    "\n",
    "## Assignment objectives\n",
    "\n",
    "In this assignment, you will develop a POS tagger using pytorch. You will:\n",
    "\n",
    "1. Read in training, development and test data using `torchtext`.\n",
    "1. Implement a baseline majority class tagger.\n",
    "1. Numericalize data (i.e. transform sentences and words into `torch.Tensor` objects).\n",
    "1. Develop a BiLSTM POS tagger.  \n",
    "\n",
    "The [`pytorch` documentation]() will be useful in this lab.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "You will need to install the Python modules `torchtext`, `torch` and `numpy`. The easiest way to do this is using `anaconda` or `pip`.\n",
    "\n",
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "\n",
    "* Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "* Be sure to follow the general lab instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Reading in data using torchtext\n",
    "\n",
    "We will now read in training development and test sets using the `torchtext` library. The `torchtext` can considerably simplify your code. If you need a tutorial on the torchtext library, please check [this one](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/) and please have a look at [Roger's torchtext workshop](https://github.ubc.ca/MDS-CL-2019-20/COLX_525_morphology_students/blob/master/pytorch_workshop/workshop1.ipynb).\n",
    "\n",
    "Let's start by importing the relevant classes from `torchtext` and setting up some important constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, NestedField\n",
    "from torchtext.datasets import SequenceTaggingDataset \n",
    "\n",
    "# The padding symbol is used when words are encoded into character sequences, the unknown symbol is used for input \n",
    "# characters, which were not attested in the training set. We also have start & end of sequence symbols which are \n",
    "# appended both to the start and end of sentences and character sequences.\n",
    "PAD=\"<pad>\"\n",
    "UNK=\"<unk>\"\n",
    "START=\"<start>\"\n",
    "END=\"<end>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [Universal Dependencies](https://universaldependencies.org/) or UD dataset for English. Sentences in UD datasets are encoded for dependency syntax which means that they are marked for POS, morphological features, syntactic dependencies (i.e. head words) and syntactic role (like predicate and subject). Below you can see an example of a sentence in UD annotation also called CoNLL-U format:\n",
    "```\n",
    "1   This    this    DET     DT    Number=Sing|PronType=Dem                                2   det     _       _\n",
    "2   item    item    NOUN    NN    Number=Sing                                             6   nsubj   _       _\n",
    "3   is      be      AUX     VBZ   Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   6   cop     _       _\n",
    "4   a       a       DET     DT    Definite=Ind|PronType=Art                               6   det     _       _\n",
    "5   small   small   ADJ     JJ    Degree=Pos                                              6   amod    _       _\n",
    "6   one     one     NOUN    NN    Number=Sing                                             0   root    _       _\n",
    "7   and     and     CCONJ   CC    _                                                       9   cc      _       _\n",
    "8   easily  easily  ADV     RB    _                                                       9   advmod  _       _\n",
    "9   missed  miss    VERB    VBN   Tense=Past|VerbForm=Part                                6   conj    _       _\n",
    "10  .       .       PUNCT   .     _                                                       6   punct   _       _\n",
    "```\n",
    "\n",
    "You will now implement a `torchtext.Dataset` which reads data in UD format. The dataset class `UDData` is a subclass of [`torchtext.datasets.SequenceTaggingDataset`](https://torchtext.readthedocs.io/en/latest/datasets.html#sequence-tagging) because our inputs are sentences consisting of several word forms. Your task is to make `UDData.splits` return a training, development and test set containing examples which have the members `word`, `char` and `pos`. For the following sentence:\n",
    "\n",
    "```\n",
    "1   This       this      PRON   PRN   Number=Sing|PronType=Dem                                2   nsubj   _   _\n",
    "2   is         be        AUX    AUX   Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   4   cop     _   _\n",
    "3   a          a         DET    DET   _                                                       4   det     _   _\n",
    "4   sentence   sentence  NOUN   NOUN  Number=Sing                                             0   root    _   _\n",
    "```\n",
    "\n",
    "your code should produce an example `ex` with the following members:\n",
    "```\n",
    "ex.word == [\"This\", \"is\", \"a\", \"sentence\"]\n",
    "ex.char == [[\"T\",\"h\",\"i\",\"s\"], [\"i\",\"s\"], [\"a\"], [\"s\",\"e\",\"n\",\"t\",\"e\",\"n\",\"c\",\"e\"]]\n",
    "ex.pos  == [\"PRON\", \"AUX\", \"DET\", \"NOUN\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1 Defining `torchtext.data.Field` objects and downloading data\n",
    "rubric={accuracy:10}\n",
    "\n",
    "The first thing you need to do is to define appropriate `torchtext.data.Field` objects for the member fields. You need to define `WORD` for the word forms in the sentence (like `This`), `CHAR` for word forms which have been split into characters (like `[\"T\",\"h\",\"i\",\"s\"]`) and `POS` for POS tags (like `NOUN`)\n",
    "\n",
    "The field `CHAR` is special because it needs to perform two tokenizations. `CHAR` first tokenizes the sentence into a sequence of words and then tokenizes each word into a sequence of characters. This can be done using a [`torchtext.data.NestedField`](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.NestedField). The idea here is that you will first define a regular `Field` which will tokenize a word form into characters and then define a `NestedField` which takes your word field as argument (more details in Roger's [torchtext workshop](https://github.ubc.ca/MDS-CL-2019-20/COLX_525_morphology_students/blob/master/pytorch_workshop/workshop1.ipynb)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "WORD = Field(sequential=True,\n",
    "            pad_token = PAD,\n",
    "            unk_token = UNK,\n",
    "        init_token = START,\n",
    "            eos_token = END)\n",
    "NESTING_FIELD = Field(tokenize=list,\n",
    "                     pad_token = PAD,\n",
    "            unk_token = UNK,\n",
    "        init_token = START,\n",
    "            eos_token = END)\n",
    "CHAR = NestedField(nesting_field=NESTING_FIELD, include_lengths = True)\n",
    "POS = Field(sequential=True)\n",
    "#             unk_token = UNK,\n",
    "#         init_token = START,\n",
    "#             eos_token = END)\n",
    "\n",
    "\n",
    "# your code here\n",
    "\n",
    "# Your fields should pass the following assertions: \n",
    "assert(WORD.preprocess(\"This is a sentence\") == [\"This\", \"is\", \"a\", \"sentence\"]) \n",
    "assert(CHAR.preprocess(\"This is a sentence\") == [[\"T\", \"h\", \"i\", \"s\"], \n",
    "                                                 [\"i\", \"s\"], \n",
    "                                                 [\"a\"], \n",
    "                                                 [\"s\", \"e\", \"n\", \"t\", \"e\", \"n\", \"c\", \"e\"]])\n",
    "assert(POS.preprocess(\"PRON AUX DET NOUN\") == [\"PRON\", \"AUX\", \"DET\", \"NOUN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to run the following code to download and read Universal Dependencies data. The data will be downloaded into a directory `data` the first time you run this code so you'll need a network connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
      "[['A', 'l'], ['-'], ['Z', 'a', 'm', 'a', 'n'], [':'], ['A', 'm', 'e', 'r', 'i', 'c', 'a', 'n'], ['f', 'o', 'r', 'c', 'e', 's'], ['k', 'i', 'l', 'l', 'e', 'd'], ['S', 'h', 'a', 'i', 'k', 'h'], ['A', 'b', 'd', 'u', 'l', 'l', 'a', 'h'], ['a', 'l'], ['-'], ['A', 'n', 'i'], [','], ['t', 'h', 'e'], ['p', 'r', 'e', 'a', 'c', 'h', 'e', 'r'], ['a', 't'], ['t', 'h', 'e'], ['m', 'o', 's', 'q', 'u', 'e'], ['i', 'n'], ['t', 'h', 'e'], ['t', 'o', 'w', 'n'], ['o', 'f'], ['Q', 'a', 'i', 'm'], [','], ['n', 'e', 'a', 'r'], ['t', 'h', 'e'], ['S', 'y', 'r', 'i', 'a', 'n'], ['b', 'o', 'r', 'd', 'e', 'r'], ['.']]\n",
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "class UDData(SequenceTaggingDataset):\n",
    "    \"\"\"\n",
    "    Universal Dependencies .\n",
    "    Download original at http://universaldependencies.org/\n",
    "    License: http://creativecommons.org/licenses/by-sa/4.0/\n",
    "    \n",
    "    UDData defines a data loader and reader for UD treebanks. Since we inherit \n",
    "    SequenceTaggingDataset, the only method we need to define is splits which \n",
    "    returns the training, development and test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    urls = ['https://mpsilfve.github.io/assets/uddata.zip']\n",
    "    dirname = 'uddata'\n",
    "    name = 'uddata'\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, language, root=\"data\", **kwargs):\n",
    "        \"\"\"\n",
    "        Downloads and reads Universal Dependencies Version 2 data. The function \n",
    "        returns three torchtext.data.Data objects: train, dev and test which \n",
    "        contain torchtext.data.Example objects.\n",
    "        \n",
    "        The language parameter should be set to \"en\" for English, \"es\" for \n",
    "        Spanish and \"fi\" for Finnish.\n",
    "        \n",
    "        The variable FIELDS determines how UDData treats the fields in the \n",
    "        CoNLL-U format. It consists of 10 fields corresponding to each of the 10 \n",
    "        fields in the CoNLL-U format. We are interested in field 1 (word form) and 3 \n",
    "        (POS tag). We don't want to extract any information for the \n",
    "        remaining fields. \n",
    "    \n",
    "        Each entry in FIELDS is a pair (name, field) where field refers to the \n",
    "        torchtext.data.Field object that handles the information stored in this \n",
    "        field. The information is stored as the variable \n",
    "        torchtext.data.Example.name. \n",
    "        \n",
    "        Field 1 is special because we extract two kinds of information: the word \n",
    "        form as a monolithic token and the same word form as a character sequence. \n",
    "        That is why FIELDS[1] extracts two name values: 'word' and 'char'.  \n",
    "        \"\"\"\n",
    "        FIELDS = ((None,None), \n",
    "                  (('word','char'), (WORD, CHAR)), \n",
    "                  (None,None), \n",
    "                  ('pos', POS), \n",
    "                  (None,None), \n",
    "                  (None,None), \n",
    "                  (None,None),\n",
    "                  (None,None),\n",
    "                  (None,None))\n",
    "        \n",
    "        return super(UDData, cls).splits(\n",
    "                fields=FIELDS, \n",
    "                root=root, \n",
    "                train=\"%s-ud-train.conllu.head\" % language, \n",
    "                validation=\"%s-ud-dev.conllu\" % language,\n",
    "                test=\"%s-ud-test.conllu\" % language, **kwargs)\n",
    "    \n",
    "# Read Universal Depandencies v2 training, development and test sets for English.\n",
    "train, dev, test = UDData.splits(language=\"en\")\n",
    "\n",
    "# Print the first example in the English training set.\n",
    "ex = next(iter(iter(iter(train))))\n",
    "print(ex.word)\n",
    "print(ex.char)\n",
    "print(ex.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2 Build Vocabularies\n",
    "\n",
    "rubric={accuracy:2}\n",
    "\n",
    "You should now construct your word, character and POS vocabularies from the **training data** by calling the `build_vocab` function for the approporiate fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "WORD.build_vocab(train)\n",
    "CHAR.build_vocab(train)\n",
    "POS.build_vocab(train)\n",
    "\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many word tokens does the `WORD` vocabulary contain? What about `CHAR` and `POS`? Print the number of tokens in each vocabulary. (There should be substantially more word tokens than characters or POS tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: 4740\n",
      "CHAR: 96\n",
      "POS: 19\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "print(\"WORD:\",len(WORD.vocab.stoi))\n",
    "print(\"CHAR:\",len(CHAR.vocab.stoi))\n",
    "print(\"POS:\",len(POS.vocab.stoi))\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Simple baseline tagger\n",
    "\n",
    "To be able to gauge the performance of our deep learning tagger, we'll now implement a baseline majority label classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Counting tags\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "As a first step, you will count the occurrences of different POS tags for each word in the **training data**. These counts will be stored in `tag_counts` below. For exaple, `tag_counts[\"this\"][\"PRON\"]` should tell you how many times the word \"this\" was tagged `PRON` in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# A counter for POS tags. tag_counts[wf][pos] should denote the number of times we saw the word wf with \n",
    "# POS tag pos in the training data.\n",
    "tag_counts = defaultdict(Counter)\n",
    "\n",
    "# Populate tag_counts with the counts of different POS tags for each word type in the training data. \n",
    "# your code here\n",
    "\n",
    "for d in train:\n",
    "    words = d.word\n",
    "    tags = d.pos\n",
    "    for word, tag in zip(words, tags):\n",
    "        tag_counts[word][tag] += 1\n",
    "    \n",
    "\n",
    "# your code here\n",
    "\n",
    "# A few assertions to make sure that your code is working properly.\n",
    "assert(tag_counts[\"this\"][\"DET\"] == 46)\n",
    "assert(tag_counts[\"this\"][\"PRON\"] == 20)\n",
    "assert(tag_counts[\"this\"][\"ADV\"] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2 Tagging the development data\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "The next step is to tag the development data. For each example in the development set, you should append a list of predicted POS tags to `output_tags`. \n",
    "\n",
    "For each word in an example, output its most common tag given by `tag_counts`. For OOV (out-of-vocabulary) words which are missing from `tag_counts`, you can predict `NOUN`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tags = []\n",
    "\n",
    "for ex in dev:\n",
    "    output_tags.append([])\n",
    "    for wf in ex.word:\n",
    "        # your code here\n",
    "        if wf in tag_counts:\n",
    "            output_tags[-1].append(max(tag_counts[wf], key = tag_counts[wf].get))\n",
    "        else:\n",
    "            output_tags[-1].append(\"NOUN\")\n",
    "        \n",
    "        # your code here\n",
    "    #print(output_tags[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `accuracy` function below, you can now print the baseline tagging accuracy on the development set. It should be around 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for baseline majority class tagger: 77.36599331954828\n"
     ]
    }
   ],
   "source": [
    "def accuracy(sys,gold):\n",
    "    \"\"\"\n",
    "    Function for evaluating tagging accuracy w.r.t. a gold standard test set (gold).\n",
    "    \"\"\"\n",
    "    assert(len(sys) == len(gold))\n",
    "    corr = 0\n",
    "    tot = 0\n",
    "    for s, g in zip(sys,gold):\n",
    "        assert(len(s) == len(g.pos))\n",
    "        corr += sum([1 if x==y else 0 for x,y in zip(s,g.pos)])\n",
    "        tot += len(s)\n",
    "    return corr * 100.0 / tot\n",
    "\n",
    "print(\"Accuracy for baseline majority class tagger:\",accuracy(output_tags,dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Numericalizing data\n",
    "\n",
    "rubric={accuracy:10}\n",
    "\n",
    "Now, you should define iterators for the training data, development data and test data. For efficiency reasons, `train_iter` should be a [`BucketIterator`](https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator) which will sort examples according to length. If you implement batching (which is not a part of this lab), using `BucketIterator` will result in less padding which means faster runtime. `dev_iter` and `test_iter` should be regular [`Iterator`](https://torchtext.readthedocs.io/en/latest/data.html#iterator) objects because we don't want to permute the examples in the development and test sets. Again, please check Roger's [torchtext workshop](https://github.ubc.ca/MDS-CL-2019-20/COLX_525_morphology_students/blob/master/pytorch_workshop/workshop1.ipynb) for details.\n",
    "\n",
    "Note that `train_iter` should shuffle examples between epochs but `dev_iter` and `test_iter` should not in order to retain the correct order of development and test examples for evaluation of accuracy. None of the iterators should repeat over multiple epochs. As `device` you should use `\"cpu\"` unless you have access to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "# your code here\n",
    "\n",
    "train_iter = BucketIterator(train,\n",
    "                          batch_size=1,\n",
    "                          sort_key=len,\n",
    "                          shuffle=True,\n",
    "                         device = \"cpu\")\n",
    "\n",
    "dev_iter, test_iter = Iterator.splits((dev, test),\n",
    "                                     batch_sizes=(1, 1),\n",
    "                                     sort=False,\n",
    "                                     shuffle=False,\n",
    "                                    device = \"cpu\")\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you understand the contents of the fields `ex.pos`, `ex.word` and `ex.char`. You just need to run the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first example from the training set:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 1 from UDDATA]\n",
      "\t[.pos]:[torch.LongTensor of size 27x1]\n",
      "\t[.word]:[torch.LongTensor of size 29x1]\n",
      "\t[.char]:('[torch.LongTensor of size 1x27x10]', '[torch.LongTensor of size 1]', '[torch.LongTensor of size 1x27]')\n",
      "\n",
      "Each example contains a vector of POS tags ex.pos having dimension (sentence_length,1):\n",
      "\n",
      "tensor([[11],\n",
      "        [11],\n",
      "        [ 9],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [10],\n",
      "        [10],\n",
      "        [ 8],\n",
      "        [ 5],\n",
      "        [ 9],\n",
      "        [12],\n",
      "        [ 9],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [ 6],\n",
      "        [13],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [ 3],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [ 4],\n",
      "        [ 3]])\n",
      "torch.Size([27, 1])\n",
      "\n",
      "Each example contains a vector of word tokens ex.word having dimension (sentence_length+2,1)\n",
      "The +2 stems from START symbol at the beginning of the sentence (WORD.vocab.stoi[START] == 2)\n",
      "and END symbol at the end of the sentence (WORD.vocab.stoi[END] == 3):\n",
      "\n",
      "tensor([[  2],\n",
      "        [244],\n",
      "        [251],\n",
      "        [108],\n",
      "        [ 35],\n",
      "        [ 19],\n",
      "        [573],\n",
      "        [826],\n",
      "        [ 23],\n",
      "        [ 38],\n",
      "        [156],\n",
      "        [ 18],\n",
      "        [263],\n",
      "        [  8],\n",
      "        [ 12],\n",
      "        [109],\n",
      "        [114],\n",
      "        [163],\n",
      "        [ 47],\n",
      "        [103],\n",
      "        [  9],\n",
      "        [596],\n",
      "        [ 25],\n",
      "        [221],\n",
      "        [  5],\n",
      "        [137],\n",
      "        [120],\n",
      "        [  6],\n",
      "        [  3]])\n",
      "torch.Size([29, 1])\n",
      "\n",
      "Each example contains a tensor of character strings ex.char[0] having dimension (1,sentence_length,max_word_length+2)\n",
      "The tensor is big enough to fit all tokens in the sentence. Again, +2 stems from the sequence initial\n",
      "symbol START (CHAR.vocab.stoi[START] == 2) and sequence final symbol END (CHAR.vocab.stoi[END] == 3)\n",
      "which are appended to each word. All words are padded to the same length using the symbol PAD\n",
      "(CHAR.vocab.stoi[PAD] == 1):\n",
      "\n",
      "tensor([[[ 2, 31,  9,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 18,  5, 11,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 20, 12,  5,  6,  3,  1,  1,  1,  1],\n",
      "         [ 2,  6, 12,  4, 21,  3,  1,  1,  1,  1],\n",
      "         [ 2, 12,  5, 25,  4,  3,  1,  1,  1,  1],\n",
      "         [ 2, 14,  9,  7,  4,  3,  1,  1,  1,  1],\n",
      "         [ 2,  6,  9, 19,  4,  6, 12,  4, 11,  3],\n",
      "         [ 2, 12,  5, 10,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 23,  4,  4,  7,  3,  1,  1,  1,  1],\n",
      "         [ 2, 19,  9,  9, 14,  3,  1,  1,  1,  1],\n",
      "         [ 2, 18,  9, 11,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 15, 10,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2,  5,  7, 14,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2,  6, 12,  5,  6,  3,  1,  1,  1,  1],\n",
      "         [ 2, 61, 10,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 12,  9, 20,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 20,  4,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 20,  9, 15, 13, 14,  3,  1,  1,  1],\n",
      "         [ 2, 13,  8, 27,  4,  3,  1,  1,  1,  1],\n",
      "         [ 2,  6,  9,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 27,  4,  4, 22,  3,  1,  1,  1,  1],\n",
      "         [ 2,  8,  6,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 61, 61,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 26,  3,  1,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 10,  5, 21, 10,  3,  1,  1,  1,  1],\n",
      "         [ 2, 53,  5, 11, 55,  5,  8,  3,  1,  1],\n",
      "         [ 2, 24,  3,  1,  1,  1,  1,  1,  1,  1]]])\n",
      "torch.Size([1, 27, 10])\n",
      "\n",
      "Additionally we get the length of each input word form ex.char[2] in a (1,sentence_length) tensor:\n",
      "\n",
      "tensor([[ 4,  5,  6,  6,  6,  6, 10,  5,  6,  6,  5,  4,  5,  6,  4,  5,  4,  7,\n",
      "          6,  4,  6,  4,  4,  3,  6,  8,  3]])\n"
     ]
    }
   ],
   "source": [
    "# ex represents the first sentence in the training set. \n",
    "ex = next(iter(train_iter))\n",
    "\n",
    "print(\"Here is the first example from the training set:\")\n",
    "print(ex)\n",
    "print(\"\\nEach example contains a vector of POS tags ex.pos having dimension (sentence_length,1):\\n\")\n",
    "print(ex.pos)\n",
    "print(ex.pos.size())\n",
    "print(\"\\nEach example contains a vector of word tokens ex.word having dimension (sentence_length+2,1)\")\n",
    "print(\"The +2 stems from START symbol at the beginning of the sentence (WORD.vocab.stoi[START] == %u)\" \n",
    "      % WORD.vocab.stoi[START])\n",
    "print(\"and END symbol at the end of the sentence (WORD.vocab.stoi[END] == %u):\\n\" % WORD.vocab.stoi[END])\n",
    "print(ex.word)\n",
    "print(ex.word.size())\n",
    "print(\"\\nEach example contains a tensor of character strings ex.char[0] having dimension (1,sentence_length,max_word_length+2)\")\n",
    "print(\"The tensor is big enough to fit all tokens in the sentence. Again, +2 stems from the sequence initial\")\n",
    "print(\"symbol START (CHAR.vocab.stoi[START] == %u) and sequence final symbol END (CHAR.vocab.stoi[END] == %u)\"\n",
    "      % (CHAR.vocab.stoi[START], CHAR.vocab.stoi[END]))\n",
    "print(\"which are appended to each word. All words are padded to the same length using the symbol PAD\")\n",
    "print(\"(CHAR.vocab.stoi[PAD] == %s):\\n\" % CHAR.vocab.stoi[PAD])\n",
    "chars, word_count, char_lengths = ex.char      \n",
    "print(chars)\n",
    "print(chars.size())\n",
    "print(\"\\nAdditionally we get the length of each input word form ex.char[2] in a (1,sentence_length) tensor:\\n\")\n",
    "print(char_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: The POS tagger\n",
    "\n",
    "In this exercise, you will build a basic BiLSTM POS tagger. The tagger:\n",
    "\n",
    "1. Embeds word tokens in the input sentence.\n",
    "2. Passes the embeddings through a bidirectional LSTM layer.\n",
    "3. Predicts POS tags using a feed-forward network and log softmax layer.\n",
    "\n",
    "When you are implementing the POS tagger, remember to always keep track of the input and output sizes of all of you tensors. It is very important to check that these are correct. It is also important to understand what your dimensions refer to.\n",
    "\n",
    "Let's start by loading a few necessary libraries and setting hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.functional import log_softmax, relu\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from random import random, seed, shuffle\n",
    "\n",
    "# Ensure reproducible results by setting random seeds to 0.\n",
    "seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import re\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(WORD.vocab.stoi)\n",
    "EMBEDDING_DIM=300\n",
    "RNN_HIDDEN_DIM=50\n",
    "RNN_LAYERS=1\n",
    "BATCH_SIZE=10\n",
    "EPOCHS=5\n",
    "\n",
    "# Maximum length of generated output word forms.\n",
    "MAXWFLEN=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1: LSTM layer\n",
    "\n",
    "rubric={accuracy:15}\n",
    "\n",
    "You should implement a `BidirectionalLSTM` class which is used to encode a sequence of word embeddings into  representations. `BidirectionalLSTM` encapsulates two LSTM networks: `self.forward_rnn` and `self.backward_rnn` which you should initialize in `BidirectionalLSTM.__init__`. Both should have:\n",
    "\n",
    "1. Embedding dimension `EMBEDDING_DIM`\n",
    "1. Hidden dimension `RNN_HIDDEN_DIM`\n",
    "1. Layer count `RNN_LAYERS`\n",
    "\n",
    "Your second task is to implement the `BidirectionalLSTM.forward` function. As argument, the function takes a `torch.Tensor` `sequence` which has size `(sequence_length,1,EMBEDDING_DIM)`. This tensor contains the word embeddings for the input sentence.  \n",
    "\n",
    "You should pass the `sequence` to `self.forward_rnn` which returns:\n",
    "\n",
    "1. a sequence $f_1,...,f_n$ of forward hidden states represented as a tensor `fwd_hss` having size `(sequence_length,1,RNN_HIDDEN_DIM)` and\n",
    "1. a pair `(fwd_hs, fwd_cs)`, where:\n",
    "   1. `fwd_hs` is the final forward hidden state having dimension `(1,1,RNN_HIDDEN_DIM)`.\n",
    "   1. `fwd_cs` is the final forward cell state having dimension `(1,1,RNN_HIDDEN_DIM)`.\n",
    "   \n",
    "You should pass the **reversed** `sequence` to `self.backward_rnn` (**HINT**: [`torch.flip`](https://pytorch.org/docs/stable/torch.html#torch.flip) can be useful here) which returns:\n",
    "\n",
    "1. a sequence $b_n,...,b_1$ of backward hidden states represented as a tensor `bwd_hss` having size `(sequence_length,1,RNN_HIDDEN_DIM)` (NOTE! the backward states are reversersed here) \n",
    "1. and a pair `(bwd_hs, bwd_cs)`, where:\n",
    "   1. `bwd_hs` is the final backward hidden state having dimension `(1,1,RNN_HIDDEN_DIM)`.\n",
    "   1. `bwd_cs` is the final backward cell state having dimension `(1,1,RNN_HIDDEN_DIM)`.\n",
    "   \n",
    "The `forward` function should return a tensor `hss` having dimension `(sequence_length, 1, 2*RNN_HIDDEN_DIM)`, where `hss[i]` represents the concatenation of the $i$th forward hidden state $f_i$ and the $i$th backward hidden state $b_i$ (**HINT**: Again `torch.flip` can be useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self):# vocab_size, emb_dim, hid_dim, n_layers):\n",
    "        super(BidirectionalLSTM,self).__init__()\n",
    "        # your code here\n",
    "        \n",
    "        self.forward_rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS)\n",
    "        self.backward_rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS)\n",
    "        #self.feedforward = nn.Linear(2*RNN_HIDDEN_DIM, len(POS.vocab.itos))\n",
    "        # your code here\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        # your code here\n",
    "\n",
    "        output_f, (hidden_f, cell_f) = self.forward_rnn(sequence)\n",
    "        output_b, (hidden_b, cell_b) = self.backward_rnn(sequence.flip(0))\n",
    "        \n",
    "        bi_output = torch.cat((output_f, output_b.flip(0)), dim = 2) \n",
    "        \n",
    "        #bi_output = bi_output[1:-1, :, :]\n",
    "        return bi_output\n",
    "        \n",
    "        # your code here\n",
    "\n",
    "# Assertions to check that your code returns objects of the correct size (not a guarantee that your code works).\n",
    "assert(BidirectionalLSTM()(torch.zeros(10,1,EMBEDDING_DIM)).size() == (10,1,2*RNN_HIDDEN_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve tagging accuracy for OOV words, we need to use word dropout. It takes two arguments:\n",
    "1. A `torch.Tensor` `sequence` of size `(sequence_length,1)` and\n",
    "1. A float `word_dropout` in the interval `[0,1]`.\n",
    "During training, the function randomly replaces words by `WORD.vocab.stoi[UNK]` with probability 'word_dropout'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_words(sequence,word_dropout):\n",
    "    seq_len, _ = sequence.size()\n",
    "    dropout_sequence = sequence.clone()\n",
    "    for i in range(1,seq_len-1):\n",
    "        if random() < word_dropout:\n",
    "            dropout_sequence[i,0] = WORD.vocab.stoi[UNK]\n",
    "    return dropout_sequence\n",
    "    \n",
    "# Assertions to check that your code returns objects of the correct size (not a guarantee that your code works).\n",
    "assert(drop_words(torch.zeros(10,1),0.5).size() == (10,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2 Sentence Encoder \n",
    "\n",
    "rubric={accuracy:15}\n",
    "\n",
    "Your next task is to build a class `SentenceEncoder` which takes an example (from `train_iter`, `dev_iter` or `test_iter`) as input and returns a sequence of LSTM hidden states given by `BidirectionalLSTM`.\n",
    "\n",
    "You first task is to initialize the `SentenceEncoder` class. You will initialize 3 class-members:\n",
    "1. `self.vocabulary` which is just an alias for `WORD.vocab.stoi`.\n",
    "1. `self.embeddings` which is a `torch.nn.Embedding` having input dimension `len(self.vocabulary)` and output dimension `EMBEDDING_DIM`.\n",
    "1. `self.rnn` which is a `BidirectionalLSTM` object.\n",
    "\n",
    "You should them implement `SentenceEncoder.forward` which takes as example `ex` as input. Additionally, it takes another parameter `word_dropout` which is the probability for word dropout on the sentence `ex`. The function should:\n",
    "1. Perform word dropout on `ex` by calling the `drop_words` function above.\n",
    "1. Embed the resulting tensor resulting in a tensor `embedded`.\n",
    "1. Run `self.rnn` on embedded.\n",
    "1. Return the resulting representation tensor. However, `ex.word` represents a sentence where we have appended an initial symbol `START` and final symbol `END`. You need to therefore clip the first and last representation vector before returning the output of `self.rnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentenceEncoder,self).__init__()\n",
    "\n",
    "        # your code here\n",
    "        \n",
    "        self.vocabulary = WORD.vocab.stoi\n",
    "        self.embedding = nn.Embedding(len(self.vocabulary), EMBEDDING_DIM)\n",
    "        self.rnn = BidirectionalLSTM()\n",
    "        \n",
    "        #your code here\n",
    "        \n",
    "    def forward(self,ex,word_dropout):\n",
    "        # your code here\n",
    "        \n",
    "        ex = drop_words(ex.word,word_dropout)\n",
    "        embedded = self.embedding(ex)\n",
    "        output = self.rnn(embedded)\n",
    "        output = output[1:-1, :, :]\n",
    "        return output\n",
    "        \n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "sentence_length = ex.word.size()[0] - 2\n",
    "assert(SentenceEncoder()(ex,0.5).size() == (sentence_length, 1, 2*RNN_HIDDEN_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.3: Prediction Layer\n",
    "\n",
    "rubric={accuracy:15}\n",
    "\n",
    "Your next task is to implement a feed-forward network `FeedForward` which is used to predict tags from LSTM representations. The constructor `feedForward.__init__` takes two arguments `input_dim` and `output_dim` representing the input and output dimension of the network, respectively. \n",
    "Your first task is to complete the function `FeedForward.__init__` by initializing two linear layers:\n",
    "1. `self.linear1` having input dimension `input_dim` and output dimension `input_dim` and\n",
    "2. `self.linear2` having input dimension `input_dim` and output dimension `output_dim`.\n",
    "\n",
    "Your second task is to implement the function `FeedForward.forward`. As input, the function takes `tensor` which is a torch.Tensor object having size `(sequence_length, 1, input_dim)`. It then:\n",
    "1. Applies `self.linear1` followed by a ReLU activation function on `tensor` and\n",
    "2. then passes the result through `self.linear2` and a `log_softmax` layer and finally returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # your code here\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim)\n",
    "        self.linear2 = nn.Linear(input_dim, output_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.LogSoftmax(dim = 2)\n",
    "        # your code here\n",
    "        \n",
    "    def forward(self,tensor):\n",
    "        # your code here\n",
    "        output = self.linear1(tensor)\n",
    "        output = self.act1(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.act2(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "# Assertions to check that your code returns objects of the correct size (not a guarantee that your code works).\n",
    "assert(FeedForward(2*RNN_HIDDEN_DIM,100)(torch.zeros(10,1,2*RNN_HIDDEN_DIM)).size() == (10,1,100))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging sentences and training the model\n",
    "\n",
    "Now it's time to put together all the components that you built so far. `SimplePOSTagger` is a wrapper around a `SentenceEncoder` and `FeedForward` layer. It has a `forward` method which returns a tensor `res` where `res[i,j]` represents the log probability of tag `POS.itos[j]` for the word at position `i` in our input sentences. \n",
    "\n",
    "The function `tag` gets POS tags for a dataset `data`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePOSTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplePOSTagger,self).__init__()\n",
    "        self.tagset_size = len(POS.vocab.itos)\n",
    "        \n",
    "        self.sentence_encoder = SentenceEncoder()\n",
    "        self.hidden2tag = FeedForward(2*RNN_HIDDEN_DIM,self.tagset_size)\n",
    "        \n",
    "    def forward(self,ex, word_dropout=0):\n",
    "        states = self.sentence_encoder(ex,word_dropout)\n",
    "        return self.hidden2tag(states)\n",
    "\n",
    "    def tag(self,data):\n",
    "        with torch.no_grad():\n",
    "            results = []\n",
    "            for ex in data:\n",
    "                tags = self(ex).argmax(dim=2).squeeze(1)\n",
    "                results.append([POS.vocab.itos[i] for i in tags])\n",
    "            return results\n",
    "        \n",
    "pos_size = len(POS.vocab.itos)\n",
    "assert(SimplePOSTagger()(ex).size() == (ex.word.size()[0]-2,1,pos_size))\n",
    "assert(len(SimplePOSTagger().tag([ex])[0]) == ex.word.size()[0] -2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with the `SimplePOSTagger` class, you can now train your tagger using the following code. You should get to around 75% tagging accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Example 1000 of 1000\n",
      "Average loss per example: 1.2292\n",
      "Development accuracy: 71.78\n",
      "Epoch 2: Example 1000 of 1000\n",
      "Average loss per example: 0.5045\n",
      "Development accuracy: 75.11\n",
      "Epoch 3: Example 1000 of 1000\n",
      "Average loss per example: 0.2637\n",
      "Development accuracy: 76.44\n",
      "Epoch 4: Example 1000 of 1000\n",
      "Average loss per example: 0.1891\n",
      "Development accuracy: 75.29\n",
      "Epoch 5: Example 1000 of 1000\n",
      "Average loss per example: 0.1542\n",
      "Development accuracy: 77.68\n"
     ]
    }
   ],
   "source": [
    "tagger = SimplePOSTagger()\n",
    "optimizer = Adam(tagger.parameters())\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "    for i,ex in enumerate(train_iter):\n",
    "        print(\"Epoch %u: Example %u of %u\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "        tagger.zero_grad()\n",
    "        output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "        gold = ex.pos.squeeze(dim=1)\n",
    "        loss = loss_function(output,gold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_loss += loss.detach().numpy()\n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/len(train_iter)))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % accuracy(sys_dev, dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that this is almost the same accuracy as for our baseline model. You can get a bit higher if you raise the number of epochs. If you really want better accuracy, you'll have to implement a character-level model and use pretrained embeddings. These can get you up to 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
