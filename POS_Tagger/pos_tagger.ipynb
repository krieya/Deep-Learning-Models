{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "Develop POS tagging using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data using torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, NestedField\n",
    "from torchtext.datasets import SequenceTaggingDataset \n",
    "\n",
    "PAD=\"<pad>\"\n",
    "UNK=\"<unk>\"\n",
    "START=\"<start>\"\n",
    "END=\"<end>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [Universal Dependencies](https://universaldependencies.org/) or UD dataset for English. Sentences in UD datasets are encoded for dependency syntax which means that they are marked for POS, morphological features, syntactic dependencies (i.e. head words) and syntactic role (like predicate and subject). Below you can see an example of a sentence in UD annotation also called CoNLL-U format:\n",
    "```\n",
    "1   This    this    DET     DT    Number=Sing|PronType=Dem                                2   det     _       _\n",
    "2   item    item    NOUN    NN    Number=Sing                                             6   nsubj   _       _\n",
    "3   is      be      AUX     VBZ   Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   6   cop     _       _\n",
    "4   a       a       DET     DT    Definite=Ind|PronType=Art                               6   det     _       _\n",
    "5   small   small   ADJ     JJ    Degree=Pos                                              6   amod    _       _\n",
    "6   one     one     NOUN    NN    Number=Sing                                             0   root    _       _\n",
    "7   and     and     CCONJ   CC    _                                                       9   cc      _       _\n",
    "8   easily  easily  ADV     RB    _                                                       9   advmod  _       _\n",
    "9   missed  miss    VERB    VBN   Tense=Past|VerbForm=Part                                6   conj    _       _\n",
    "10  .       .       PUNCT   .     _                                                       6   punct   _       _\n",
    "```\n",
    "\n",
    "We will now implement a `torchtext.Dataset` which reads data in UD format. The dataset class `UDData` is a subclass of [`torchtext.datasets.SequenceTaggingDataset`](https://torchtext.readthedocs.io/en/latest/datasets.html#sequence-tagging) because our inputs are sentences consisting of several word forms. Your task is to make `UDData.splits` return a training, development and test set containing examples which have the members `word`, `char` and `pos`. For the following sentence:\n",
    "\n",
    "```\n",
    "1   This       this      PRON   PRN   Number=Sing|PronType=Dem                                2   nsubj   _   _\n",
    "2   is         be        AUX    AUX   Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   4   cop     _   _\n",
    "3   a          a         DET    DET   _                                                       4   det     _   _\n",
    "4   sentence   sentence  NOUN   NOUN  Number=Sing                                             0   root    _   _\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining `torchtext.data.Field` objects and downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WORD = Field(sequential=True,\n",
    "            pad_token = PAD,\n",
    "            unk_token = UNK,\n",
    "        init_token = START,\n",
    "            eos_token = END)\n",
    "NESTING_FIELD = Field(tokenize=list,\n",
    "                     pad_token = PAD,\n",
    "            unk_token = UNK,\n",
    "        init_token = START,\n",
    "            eos_token = END)\n",
    "CHAR = NestedField(nesting_field=NESTING_FIELD, include_lengths = True)\n",
    "POS = Field(sequential=True)\n",
    "\n",
    "\n",
    "assert(WORD.preprocess(\"This is a sentence\") == [\"This\", \"is\", \"a\", \"sentence\"]) \n",
    "assert(CHAR.preprocess(\"This is a sentence\") == [[\"T\", \"h\", \"i\", \"s\"], \n",
    "                                                 [\"i\", \"s\"], \n",
    "                                                 [\"a\"], \n",
    "                                                 [\"s\", \"e\", \"n\", \"t\", \"e\", \"n\", \"c\", \"e\"]])\n",
    "assert(POS.preprocess(\"PRON AUX DET NOUN\") == [\"PRON\", \"AUX\", \"DET\", \"NOUN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and read Universal Dependencies data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
      "[['A', 'l'], ['-'], ['Z', 'a', 'm', 'a', 'n'], [':'], ['A', 'm', 'e', 'r', 'i', 'c', 'a', 'n'], ['f', 'o', 'r', 'c', 'e', 's'], ['k', 'i', 'l', 'l', 'e', 'd'], ['S', 'h', 'a', 'i', 'k', 'h'], ['A', 'b', 'd', 'u', 'l', 'l', 'a', 'h'], ['a', 'l'], ['-'], ['A', 'n', 'i'], [','], ['t', 'h', 'e'], ['p', 'r', 'e', 'a', 'c', 'h', 'e', 'r'], ['a', 't'], ['t', 'h', 'e'], ['m', 'o', 's', 'q', 'u', 'e'], ['i', 'n'], ['t', 'h', 'e'], ['t', 'o', 'w', 'n'], ['o', 'f'], ['Q', 'a', 'i', 'm'], [','], ['n', 'e', 'a', 'r'], ['t', 'h', 'e'], ['S', 'y', 'r', 'i', 'a', 'n'], ['b', 'o', 'r', 'd', 'e', 'r'], ['.']]\n",
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "class UDData(SequenceTaggingDataset):\n",
    "    \"\"\"\n",
    "    Universal Dependencies .\n",
    "    Download original at http://universaldependencies.org/\n",
    "    License: http://creativecommons.org/licenses/by-sa/4.0/\n",
    "    \n",
    "    UDData defines a data loader and reader for UD treebanks. Since we inherit \n",
    "    SequenceTaggingDataset, the only method we need to define is splits which \n",
    "    returns the training, development and test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    urls = ['https://mpsilfve.github.io/assets/uddata.zip']\n",
    "    dirname = 'uddata'\n",
    "    name = 'uddata'\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, language, root=\"data\", **kwargs):\n",
    "        \"\"\"\n",
    "        Downloads and reads Universal Dependencies Version 2 data. The function \n",
    "        returns three torchtext.data.Data objects: train, dev and test which \n",
    "        contain torchtext.data.Example objects.\n",
    "        \n",
    "        The language parameter should be set to \"en\" for English, \"es\" for \n",
    "        Spanish and \"fi\" for Finnish.\n",
    "        \n",
    "        The variable FIELDS determines how UDData treats the fields in the \n",
    "        CoNLL-U format. It consists of 10 fields corresponding to each of the 10 \n",
    "        fields in the CoNLL-U format. We are interested in field 1 (word form) and 3 \n",
    "        (POS tag). We don't want to extract any information for the \n",
    "        remaining fields. \n",
    "    \n",
    "        Each entry in FIELDS is a pair (name, field) where field refers to the \n",
    "        torchtext.data.Field object that handles the information stored in this \n",
    "        field. The information is stored as the variable \n",
    "        torchtext.data.Example.name. \n",
    "        \n",
    "        Field 1 is special because we extract two kinds of information: the word \n",
    "        form as a monolithic token and the same word form as a character sequence. \n",
    "        That is why FIELDS[1] extracts two name values: 'word' and 'char'.  \n",
    "        \"\"\"\n",
    "        FIELDS = ((None,None), \n",
    "                  (('word','char'), (WORD, CHAR)), \n",
    "                  (None,None), \n",
    "                  ('pos', POS), \n",
    "                  (None,None), \n",
    "                  (None,None), \n",
    "                  (None,None),\n",
    "                  (None,None),\n",
    "                  (None,None))\n",
    "        \n",
    "        return super(UDData, cls).splits(\n",
    "                fields=FIELDS, \n",
    "                root=root, \n",
    "                train=\"%s-ud-train.conllu.head\" % language, \n",
    "                validation=\"%s-ud-dev.conllu\" % language,\n",
    "                test=\"%s-ud-test.conllu\" % language, **kwargs)\n",
    "    \n",
    "# Read Universal Depandencies v2 training, development and test sets for English.\n",
    "train, dev, test = UDData.splits(language=\"en\")\n",
    "\n",
    "# Print the first example in the English training set.\n",
    "ex = next(iter(iter(iter(train))))\n",
    "print(ex.word)\n",
    "print(ex.char)\n",
    "print(ex.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabularies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD.build_vocab(train)\n",
    "CHAR.build_vocab(train)\n",
    "POS.build_vocab(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: 4740\n",
      "CHAR: 96\n",
      "POS: 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"WORD:\",len(WORD.vocab.stoi))\n",
    "print(\"CHAR:\",len(CHAR.vocab.stoi))\n",
    "print(\"POS:\",len(POS.vocab.stoi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple baseline tagger\n",
    "\n",
    "To be able to gauge the performance of our deep learning tagger, we'll now implement a baseline majority label classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# A counter for POS tags. tag_counts[wf][pos] should denote the number of times we saw the word wf with \n",
    "# POS tag pos in the training data.\n",
    "tag_counts = defaultdict(Counter)\n",
    "\n",
    "# Populate tag_counts with the counts of different POS tags for each word type in the training data. \n",
    "\n",
    "for d in train:\n",
    "    words = d.word\n",
    "    tags = d.pos\n",
    "    for word, tag in zip(words, tags):\n",
    "        tag_counts[word][tag] += 1\n",
    "        \n",
    "# assert the answer\n",
    "assert(tag_counts[\"this\"][\"DET\"] == 46)\n",
    "assert(tag_counts[\"this\"][\"PRON\"] == 20)\n",
    "assert(tag_counts[\"this\"][\"ADV\"] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging the development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tags = []\n",
    "\n",
    "for ex in dev:\n",
    "    output_tags.append([])\n",
    "    for wf in ex.word:\n",
    "        if wf in tag_counts:\n",
    "            output_tags[-1].append(max(tag_counts[wf], key = tag_counts[wf].get))\n",
    "        else:\n",
    "            output_tags[-1].append(\"NOUN\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the baseline accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for baseline majority class tagger: 77.36599331954828\n"
     ]
    }
   ],
   "source": [
    "def accuracy(sys,gold):\n",
    "    \"\"\"\n",
    "    Function for evaluating tagging accuracy w.r.t. a gold standard test set (gold).\n",
    "    \"\"\"\n",
    "    assert(len(sys) == len(gold))\n",
    "    corr = 0\n",
    "    tot = 0\n",
    "    for s, g in zip(sys,gold):\n",
    "        assert(len(s) == len(g.pos))\n",
    "        corr += sum([1 if x==y else 0 for x,y in zip(s,g.pos)])\n",
    "        tot += len(s)\n",
    "    return corr * 100.0 / tot\n",
    "\n",
    "print(\"Accuracy for baseline majority class tagger:\",accuracy(output_tags,dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing data\n",
    "\n",
    "Define iterators for the training data, development data and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter = BucketIterator(train,\n",
    "                          batch_size=1,\n",
    "                          sort_key=len,\n",
    "                          shuffle=True,\n",
    "                         device = \"cpu\")\n",
    "\n",
    "dev_iter, test_iter = Iterator.splits((dev, test),\n",
    "                                     batch_sizes=(1, 1),\n",
    "                                     sort=False,\n",
    "                                     shuffle=False,\n",
    "                                    device = \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first example from the training set:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 1 from UDDATA]\n",
      "\t[.pos]:[torch.LongTensor of size 27x1]\n",
      "\t[.word]:[torch.LongTensor of size 29x1]\n",
      "\t[.char]:('[torch.LongTensor of size 1x27x10]', '[torch.LongTensor of size 1]', '[torch.LongTensor of size 1x27]')\n",
      "\n",
      "Each example contains a vector of POS tags ex.pos having dimension (sentence_length,1):\n",
      "\n",
      "tensor([[11],\n",
      "        [11],\n",
      "        [ 9],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [10],\n",
      "        [10],\n",
      "        [ 8],\n",
      "        [ 5],\n",
      "        [ 9],\n",
      "        [12],\n",
      "        [ 9],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [ 6],\n",
      "        [13],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [ 3],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [ 4],\n",
      "        [ 3]])\n",
      "torch.Size([27, 1])\n",
      "\n",
      "Each example contains a vector of word tokens ex.word having dimension (sentence_length+2,1)\n",
      "The +2 stems from START symbol at the beginning of the sentence (WORD.vocab.stoi[START] == 2)\n",
      "and END symbol at the end of the sentence (WORD.vocab.stoi[END] == 3):\n",
      "\n",
      "tensor([[  2],\n",
      "        [244],\n",
      "        [251],\n",
      "        [108],\n",
      "        [ 35],\n",
      "        [ 19],\n",
      "        [573],\n",
      "        [826],\n",
      "        [ 23],\n",
      "        [ 38],\n",
      "        [156],\n",
      "        [ 18],\n",
      "        [263],\n",
      "        [  8],\n",
      "        [ 12],\n",
      "        [109],\n",
      "        [114],\n",
      "        [163],\n",
      "        [ 47],\n",
      "        [103],\n",
      "        [  9],\n",
      "        [596],\n",
      "        [ 25],\n",
      "        [221],\n",
      "        [  5],\n",
      "        [137],\n",
      "        [120],\n",
      "        [  6],\n",
      "        [  3]])\n",
      "torch.Size([29, 1])\n",
      "\n",
      "Each example contains a tensor of character strings ex.char[0] having dimension (1,sentence_length,max_word_length+2)\n",
      "The tensor is big enough to fit all tokens in the sentence. Again, +2 stems from the sequence initial\n",
      "symbol START (CHAR.vocab.stoi[START] == 2) and sequence final symbol END (CHAR.vocab.stoi[END] == 3)\n",
      "which are appended to each word. All words are padded to the same length using the symbol PAD\n",
      "(CHAR.vocab.stoi[PAD] == 1):\n",
      "\n",
      "tensor([[[ 2, 31,  9,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 18,  5, 11,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 20, 12,  5,  6,  3,  1,  1,  1,  1],\n",
      "         [ 2,  6, 12,  4, 21,  3,  1,  1,  1,  1],\n",
      "         [ 2, 12,  5, 25,  4,  3,  1,  1,  1,  1],\n",
      "         [ 2, 14,  9,  7,  4,  3,  1,  1,  1,  1],\n",
      "         [ 2,  6,  9, 19,  4,  6, 12,  4, 11,  3],\n",
      "         [ 2, 12,  5, 10,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 23,  4,  4,  7,  3,  1,  1,  1,  1],\n",
      "         [ 2, 19,  9,  9, 14,  3,  1,  1,  1,  1],\n",
      "         [ 2, 18,  9, 11,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 15, 10,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2,  5,  7, 14,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2,  6, 12,  5,  6,  3,  1,  1,  1,  1],\n",
      "         [ 2, 61, 10,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 12,  9, 20,  3,  1,  1,  1,  1,  1],\n",
      "         [ 2, 20,  4,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 20,  9, 15, 13, 14,  3,  1,  1,  1],\n",
      "         [ 2, 13,  8, 27,  4,  3,  1,  1,  1,  1],\n",
      "         [ 2,  6,  9,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 27,  4,  4, 22,  3,  1,  1,  1,  1],\n",
      "         [ 2,  8,  6,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 61, 61,  3,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 26,  3,  1,  1,  1,  1,  1,  1,  1],\n",
      "         [ 2, 10,  5, 21, 10,  3,  1,  1,  1,  1],\n",
      "         [ 2, 53,  5, 11, 55,  5,  8,  3,  1,  1],\n",
      "         [ 2, 24,  3,  1,  1,  1,  1,  1,  1,  1]]])\n",
      "torch.Size([1, 27, 10])\n",
      "\n",
      "Additionally we get the length of each input word form ex.char[2] in a (1,sentence_length) tensor:\n",
      "\n",
      "tensor([[ 4,  5,  6,  6,  6,  6, 10,  5,  6,  6,  5,  4,  5,  6,  4,  5,  4,  7,\n",
      "          6,  4,  6,  4,  4,  3,  6,  8,  3]])\n"
     ]
    }
   ],
   "source": [
    "# ex represents the first sentence in the training set. \n",
    "ex = next(iter(train_iter))\n",
    "\n",
    "print(\"Here is the first example from the training set:\")\n",
    "print(ex)\n",
    "print(\"\\nEach example contains a vector of POS tags ex.pos having dimension (sentence_length,1):\\n\")\n",
    "print(ex.pos)\n",
    "print(ex.pos.size())\n",
    "print(\"\\nEach example contains a vector of word tokens ex.word having dimension (sentence_length+2,1)\")\n",
    "print(\"The +2 stems from START symbol at the beginning of the sentence (WORD.vocab.stoi[START] == %u)\" \n",
    "      % WORD.vocab.stoi[START])\n",
    "print(\"and END symbol at the end of the sentence (WORD.vocab.stoi[END] == %u):\\n\" % WORD.vocab.stoi[END])\n",
    "print(ex.word)\n",
    "print(ex.word.size())\n",
    "print(\"\\nEach example contains a tensor of character strings ex.char[0] having dimension (1,sentence_length,max_word_length+2)\")\n",
    "print(\"The tensor is big enough to fit all tokens in the sentence. Again, +2 stems from the sequence initial\")\n",
    "print(\"symbol START (CHAR.vocab.stoi[START] == %u) and sequence final symbol END (CHAR.vocab.stoi[END] == %u)\"\n",
    "      % (CHAR.vocab.stoi[START], CHAR.vocab.stoi[END]))\n",
    "print(\"which are appended to each word. All words are padded to the same length using the symbol PAD\")\n",
    "print(\"(CHAR.vocab.stoi[PAD] == %s):\\n\" % CHAR.vocab.stoi[PAD])\n",
    "chars, word_count, char_lengths = ex.char      \n",
    "print(chars)\n",
    "print(chars.size())\n",
    "print(\"\\nAdditionally we get the length of each input word form ex.char[2] in a (1,sentence_length) tensor:\\n\")\n",
    "print(char_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The POS tagger\n",
    "\n",
    "Build a basic BiLSTM POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.functional import log_softmax, relu\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from random import random, seed, shuffle\n",
    "\n",
    "# Ensure reproducible results by setting random seeds to 0.\n",
    "seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import re\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(WORD.vocab.stoi)\n",
    "EMBEDDING_DIM=300\n",
    "RNN_HIDDEN_DIM=50\n",
    "RNN_LAYERS=1\n",
    "BATCH_SIZE=10\n",
    "EPOCHS=5\n",
    "\n",
    "# Maximum length of generated output word forms.\n",
    "MAXWFLEN=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM layer\n",
    "\n",
    "Build bi-directional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BidirectionalLSTM,self).__init__()\n",
    "\n",
    "        self.forward_rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS)\n",
    "        self.backward_rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS)\n",
    "\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        output_f, (hidden_f, cell_f) = self.forward_rnn(sequence)\n",
    "        output_b, (hidden_b, cell_b) = self.backward_rnn(sequence.flip(0))\n",
    "        \n",
    "        bi_output = torch.cat((output_f, output_b.flip(0)), dim = 2) \n",
    "        \n",
    "        return bi_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve tagging accuracy for OOV words, we need to use word dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_words(sequence,word_dropout):\n",
    "    seq_len, _ = sequence.size()\n",
    "    dropout_sequence = sequence.clone()\n",
    "    for i in range(1,seq_len-1):\n",
    "        if random() < word_dropout:\n",
    "            dropout_sequence[i,0] = WORD.vocab.stoi[UNK]\n",
    "    return dropout_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentenceEncoder,self).__init__()\n",
    "        \n",
    "        self.vocabulary = WORD.vocab.stoi\n",
    "        self.embedding = nn.Embedding(len(self.vocabulary), EMBEDDING_DIM)\n",
    "        self.rnn = BidirectionalLSTM()\n",
    "        \n",
    "    def forward(self,ex,word_dropout):\n",
    "        \n",
    "        ex = drop_words(ex.word,word_dropout)\n",
    "        embedded = self.embedding(ex)\n",
    "        output = self.rnn(embedded)\n",
    "        output = output[1:-1, :, :]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim)\n",
    "        self.linear2 = nn.Linear(input_dim, output_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.LogSoftmax(dim = 2)\n",
    "        \n",
    "    def forward(self,tensor):\n",
    "        output = self.linear1(tensor)\n",
    "        output = self.act1(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.act2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging sentences and training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePOSTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplePOSTagger,self).__init__()\n",
    "        self.tagset_size = len(POS.vocab.itos)\n",
    "        \n",
    "        self.sentence_encoder = SentenceEncoder()\n",
    "        self.hidden2tag = FeedForward(2*RNN_HIDDEN_DIM,self.tagset_size)\n",
    "        \n",
    "    def forward(self,ex, word_dropout=0):\n",
    "        states = self.sentence_encoder(ex,word_dropout)\n",
    "        return self.hidden2tag(states)\n",
    "\n",
    "    def tag(self,data):\n",
    "        with torch.no_grad():\n",
    "            results = []\n",
    "            for ex in data:\n",
    "                tags = self(ex).argmax(dim=2).squeeze(1)\n",
    "                results.append([POS.vocab.itos[i] for i in tags])\n",
    "            return results\n",
    "        \n",
    "pos_size = len(POS.vocab.itos)\n",
    "assert(SimplePOSTagger()(ex).size() == (ex.word.size()[0]-2,1,pos_size))\n",
    "assert(len(SimplePOSTagger().tag([ex])[0]) == ex.word.size()[0] -2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with the `SimplePOSTagger` class, you can now train your tagger using the following code. You should get to around 75% tagging accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Example 1000 of 1000\n",
      "Average loss per example: 1.2292\n",
      "Development accuracy: 71.78\n",
      "Epoch 2: Example 1000 of 1000\n",
      "Average loss per example: 0.5045\n",
      "Development accuracy: 75.11\n",
      "Epoch 3: Example 1000 of 1000\n",
      "Average loss per example: 0.2637\n",
      "Development accuracy: 76.44\n",
      "Epoch 4: Example 1000 of 1000\n",
      "Average loss per example: 0.1891\n",
      "Development accuracy: 75.29\n",
      "Epoch 5: Example 1000 of 1000\n",
      "Average loss per example: 0.1542\n",
      "Development accuracy: 77.68\n"
     ]
    }
   ],
   "source": [
    "tagger = SimplePOSTagger()\n",
    "optimizer = Adam(tagger.parameters())\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "    for i,ex in enumerate(train_iter):\n",
    "        print(\"Epoch %u: Example %u of %u\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "        tagger.zero_grad()\n",
    "        output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "        gold = ex.pos.squeeze(dim=1)\n",
    "        loss = loss_function(output,gold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_loss += loss.detach().numpy()\n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/len(train_iter)))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % accuracy(sys_dev, dev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
