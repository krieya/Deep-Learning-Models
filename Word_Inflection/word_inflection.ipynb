{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Inflection Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "CONLL_SIGMORPHON_DATA_PATH=\"./conll2017/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data using Torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, ReversibleField\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "# Padding symbol for batching, unknown symbol used for input characters, which were not attested in the training \n",
    "# set, and start & end of sequence symbols.\n",
    "PAD=\"<pad>\"\n",
    "UNK=\"<unk>\"\n",
    "START=\"<start>\"\n",
    "END=\"<end>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Data Fields\n",
    "\n",
    "The CoNLL-SIGMORPHON shared task datasets are in tsv (tab-separated values) format. Each line in the files has three fields: a lemma, a word form and a morphosyntactic description. Here are the ten first lines from the file `conll2017/all/task1/english-train-high`\n",
    "\n",
    "```\n",
    "malinvest       malinvest        V;NFIN\n",
    "engender        engender         V;NFIN\n",
    "stodge          stodged          V;PST\n",
    "oversew         oversew          V;NFIN\n",
    "psychoanalyse   psychoanalysing  V;V.PTCP;PRS\n",
    "foray           forayed          V;V.PTCP;PST\n",
    "DIY             DIYs             V;3;SG;PRS\n",
    "pearl           pearling         V;V.PTCP;PRS\n",
    "use             uses             V;3;SG;PRS\n",
    "strong-arm      strong-armed     V;PST\n",
    "```\n",
    "\n",
    "Our task is to read shared task datasets into `torchtext.data.Dataset` objects. The first step toward this goal is to define tokenizers for the datasets. We need to define two tokenizers `word_tok` and `msd_tok`. These should split words and lemmas into character sequences and split morphosyntactic descriptors into sequences of features like `FEAT=V` and `FEAT=V.PTCP`. Note that `msd_tok` should also prepend `FEAT=` to each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two tokenizers word_tok and tag_tok. word_tok is used for words and lemmas. msd_tok \n",
    "# is used for morphosyntactic descriptors.                                                                                                                                                  \n",
    "\n",
    "def word_tok(word):\n",
    "    '''\n",
    "    split words/lemmas into character sequences\n",
    "    '''\n",
    "    return list(word)\n",
    "\n",
    "def msd_tok(mor_des):\n",
    "    '''\n",
    "    split morphosyntactic descriptors into sequences of features\n",
    "    '''\n",
    "    \n",
    "    return [\"FEAT=\"+d for d in mor_des.split(\";\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define `torchtext.data.Field` objects for reading in word form, lemma and MSD fields in the datasets. We should define two pytorch fields `WORD` and `MSD`. The `WORD` field describes both lemmas and word forms and the `MSD` field describes morphosyntactic descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two torchtext fields WORD and MSD for handling words, lemmas and \n",
    "# morphosyntactic descriptors. These will take word_tok and msd_tok as arguments.\n",
    "\n",
    "WORD = Field(sequential=True,\n",
    "             tokenize = word_tok,\n",
    "            pad_token = PAD,\n",
    "            unk_token = UNK,\n",
    "        init_token = START,\n",
    "            eos_token = END,\n",
    "            include_lengths=True)\n",
    "MSD = Field(sequential=True,\n",
    "             tokenize = msd_tok)\n",
    "\n",
    "\n",
    "# These assertions check that your Field objects work correctly when reading the line:\n",
    "# stodge    stodged    V;PST\n",
    "assert(WORD.preprocess(\"stodged\") == [\"s\",\"t\",\"o\",\"d\",\"g\",\"e\",\"d\"])\n",
    "assert(MSD.preprocess(\"V;PST\") == [\"FEAT=V\",\"FEAT=PST\"])\n",
    "\n",
    "# Our data sets have three fields: lemma (input), word form (output) and MSD.                                                                                                                            \n",
    "datafields = [(\"input\", WORD), (\"output\", WORD), (\"msd\", MSD)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function for Reading in Data\n",
    "\n",
    "Define the function `read_data` which will read in the shared task datasets. The function takes three arguments: the target language, the data setting (\"high\" for 10,000 training examples, \"medium\" for 1,000 training examples or \"low\" for 100 training examples) and the batch size. \n",
    "\n",
    "Note that `train_iter` should shuffle examples between epochs but `dev_iter` and `test_iter` should not in order to retain the correct order of development and test examples for evaluation of accuracy. None of the iterators should repeat over multiple epochs. As `device` you should use `\"cpu\"` unless you have access to a GPU.\n",
    "\n",
    "After defining the `read_data` function, use it to read in the English shared task data for the \"medium\" setting which should give you three data iterators: `train_iter` (iterator over 1,000 English training examples), `dev_iter` (iterator over the English development data) and `test_iter` (iterator over the English test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(language,setting,batch_size=1):\n",
    "    \"\"\"Read shared task training, development and test sets for a particular language and\n",
    "       return torchtext Iterators to the data. \n",
    "    \"\"\"\n",
    "    train, dev = TabularDataset.splits(\n",
    "        path=\"%s/all/task1/\" % CONLL_SIGMORPHON_DATA_PATH,\n",
    "        train='%s-train-%s' % (language,setting),\n",
    "        validation=\"%s-dev\" % language,\n",
    "        format='tsv',\n",
    "        skip_header=True,\n",
    "        fields=datafields)\n",
    "\n",
    "    test = TabularDataset(\n",
    "        path=\"%s/answers/task1/%s-uncovered-test\" % (CONLL_SIGMORPHON_DATA_PATH,language),\n",
    "        format='tsv',\n",
    "        skip_header=True,\n",
    "        fields=datafields)\n",
    " \n",
    "    # Concatenate the lemma and MSD fields into a joint input field.\n",
    "    for data in [train, dev, test]:\n",
    "        for ex in data:\n",
    "            ex.input = ex.input + ex.msd\n",
    "    \n",
    "    # Build vocabularies                                                        \n",
    "    WORD.build_vocab(train)\n",
    "    MSD.build_vocab(train)\n",
    "\n",
    "    # Define train_iter, dev_iter and test_iter iterators over the training data, \n",
    "    # development data and test data, respectively.\n",
    "\n",
    "    train_iter = BucketIterator(train,\n",
    "                          batch_size=batch_size,\n",
    "                          sort_key=len,\n",
    "                          shuffle=True,\n",
    "                         device = \"cpu\")\n",
    "    \n",
    "    dev_iter, test_iter = Iterator.splits((dev, test),\n",
    "                                     batch_sizes=(batch_size, batch_size),\n",
    "                                     sort=False,\n",
    "                                     shuffle=False,\n",
    "                                    device = \"cpu\")\n",
    "\n",
    "    return train_iter, dev_iter, test_iter\n",
    "\n",
    "# Now call read_data to initialize train, dev and test data iterators for English. \n",
    "# Use batch_size 1.\n",
    "\n",
    "train_iter, dev_iter, test_iter = read_data(language=\"english\",\n",
    "                                            setting=\"medium\",\n",
    "                                            batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 1]\n",
      "\t[.input]:('[torch.LongTensor of size 14x1]', '[torch.LongTensor of size 1]')\n",
      "\t[.output]:('[torch.LongTensor of size 13x1]', '[torch.LongTensor of size 1]')\n",
      "\t[.msd]:[torch.LongTensor of size 2x1]\n"
     ]
    }
   ],
   "source": [
    "# Print the first training example\n",
    "example = next(iter(train_iter))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Encoder-Decoder Model\n",
    "\n",
    "This first version of the model does not implement attention. It simply uses a bidirectional encoder to encode the entire input sequence (for example, `<start> s t o d g e FEAT=V FEAT=PST <end>`) into a single hidden state vector which is then fed into a decoder network which generates the output word form (`s t o d g e d`) one symbol at a time. \n",
    "\n",
    "Let's start by loading some important requirements and defining model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.functional import log_softmax\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from random import random, seed, shuffle\n",
    "\n",
    "# Ensure reproducible results.\n",
    "seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import re\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM=50\n",
    "RNN_HIDDEN_DIM=50\n",
    "RNN_LAYERS=1\n",
    "BATCH_SIZE=10\n",
    "CHAR_DROPOUT=0.0\n",
    "EPOCHS=10\n",
    "\n",
    "# Maximum length of generated output word forms.\n",
    "MAXWFLEN=40\n",
    "\n",
    "def accuracy(sys,gold):\n",
    "    assert(len(sys) == len(gold))\n",
    "    return sum([1 if x==y else 0 for x,y in zip(sys,gold)])*100.0/len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "        def __init__(self,alphabet):\n",
    "                super(Encoder,self).__init__()\n",
    "                self.embedding = nn.Embedding(len(alphabet), EMBEDDING_DIM)\n",
    "                self.rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS, bidirectional=True)\n",
    "\n",
    "        def forward(self,ex):\n",
    "            input, _ = ex.input\n",
    "            \n",
    "            embedded = self.embedding(input)\n",
    "            # embedded = [seq_len, 1, emd_dim]\n",
    "            \n",
    "            _, hn = self.rnn(embedded)\n",
    "            \n",
    "            hn = hn[0]\n",
    "            \n",
    "            cat = hn.view(1, 1, 2 * hn.size()[2])\n",
    "            \n",
    "            return cat\n",
    "\n",
    "# An assertion to test the implementation returns a tensor of the correct size. \n",
    "assert(Encoder(WORD.vocab.stoi)(example).size() == torch.Size([1,1,2*RNN_HIDDEN_DIM]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(len(alphabet), EMBEDDING_DIM)\n",
    "        self.rnn = nn.LSTM(EMBEDDING_DIM+2*RNN_HIDDEN_DIM, RNN_HIDDEN_DIM, RNN_LAYERS, bidirectional=False)\n",
    "        self.hidden2char = nn.Linear(RNN_HIDDEN_DIM, len(alphabet))\n",
    "        \n",
    "    def forward(self,ex,encoder_hs):\n",
    "        # encoder_hs = [1, 1, 2 * hid_dim]\n",
    "        \n",
    "        output, output_length = ex.output\n",
    "        \n",
    "        embedded_output = self.embedding(output[:-1])\n",
    "        # embedded_output = [out_len - 1, 1, emb_dim]\n",
    "        \n",
    "        encoder_to_cat = encoder_hs.expand(output_length -1, 1, 2 * RNN_HIDDEN_DIM)\n",
    "                \n",
    "        decoder_input = torch.cat((embedded_output, encoder_to_cat), dim = 2)\n",
    "        # decoder_input = [out_len - 1, 1, emb_dim + 2 * hid_dim]\n",
    "        \n",
    "        decoder_hidden_states, _ = self.rnn(decoder_input)\n",
    "        # decoder_hidden_states = [out_len - 1, 1, hid_dim]\n",
    "        \n",
    "        hidden2char = self.hidden2char(decoder_hidden_states)\n",
    "        # hidden2char = [out_len -1, 1, len(alphabet)]\n",
    "        \n",
    "        distr = nn.functional.log_softmax(hidden2char, dim = 2)\n",
    "        # distr = [out_len -1, 1, len(alphabet)]\n",
    "        \n",
    "        return distr, output[1:]\n",
    "        \n",
    "    def generate(self,encoder_hs):\n",
    "        # We're not accumulating gradients during test time.\n",
    "        with torch.no_grad():\n",
    "            decoder_state = (torch.zeros(1,1,RNN_HIDDEN_DIM), torch.zeros(1,1,RNN_HIDDEN_DIM))\n",
    "            output_char = torch.LongTensor([[self.alphabet[START]]])\n",
    "            result = []\n",
    "            for _ in range(MAXWFLEN):\n",
    "\n",
    "                output_embedding = self.embedding(output_char)\n",
    "                # output_embedding = [1, 1, emd_dim]\n",
    "                \n",
    "                # encoder_hs = [1, 1, 2 * hid_dim]\n",
    "                \n",
    "                cat = torch.cat((output_embedding, encoder_hs), dim = 2)\n",
    "                \n",
    "                _, decoder_state = self.rnn(cat, decoder_state)\n",
    "                \n",
    "                hs = decoder_state[0]\n",
    "                \n",
    "                # hs = [1, 1, hid_dim]\n",
    "                \n",
    "                hid2char = self.hidden2char(hs)\n",
    "                \n",
    "                # hid2char = [1, 1, len(alphabet)]\n",
    "                \n",
    "                hid2char = nn.functional.log_softmax(hid2char, dim = 2)\n",
    "                \n",
    "                output_char = torch.argmax(hid2char, dim = 2)\n",
    "\n",
    "                result.append(output_char.numpy().tolist()[0][0])\n",
    "            \n",
    "            return result\n",
    "            \n",
    "# Assertions to test the implementation returns objects of the correct size. \n",
    "alphabet = WORD.vocab.stoi\n",
    "encoder_hs = Encoder(alphabet)(example)\n",
    "_, output_length = example.output\n",
    "alphabet_size = len(alphabet)\n",
    "\n",
    "assert(Decoder(alphabet)(example,encoder_hs)[0].size() == torch.Size([output_length - 1,1,alphabet_size]))\n",
    "assert(len(Decoder(alphabet).generate(encoder_hs)) == MAXWFLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 999 of 999\n",
      "EPOCH 1: AVG LOSS PER EX: 2.48444\n",
      "DEV ACC: 0.00%\n",
      "Example 999 of 999\n",
      "EPOCH 2: AVG LOSS PER EX: 1.85903\n",
      "DEV ACC: 0.20%\n",
      "Example 999 of 999\n",
      "EPOCH 3: AVG LOSS PER EX: 1.48870\n",
      "DEV ACC: 0.30%\n",
      "Example 999 of 999\n",
      "EPOCH 4: AVG LOSS PER EX: 1.20146\n",
      "DEV ACC: 1.50%\n",
      "Example 999 of 999\n",
      "EPOCH 5: AVG LOSS PER EX: 0.99420\n",
      "DEV ACC: 2.50%\n",
      "Example 999 of 999\n",
      "EPOCH 6: AVG LOSS PER EX: 0.83096\n",
      "DEV ACC: 3.70%\n",
      "Example 999 of 999\n",
      "EPOCH 7: AVG LOSS PER EX: 0.70516\n",
      "DEV ACC: 5.81%\n",
      "Example 999 of 999\n",
      "EPOCH 8: AVG LOSS PER EX: 0.60533\n",
      "DEV ACC: 5.71%\n",
      "Example 999 of 999\n",
      "EPOCH 9: AVG LOSS PER EX: 0.53115\n",
      "DEV ACC: 8.21%\n",
      "Example 999 of 999\n",
      "EPOCH 10: AVG LOSS PER EX: 0.45551\n",
      "DEV ACC: 9.91%\n"
     ]
    }
   ],
   "source": [
    "class WordInflector(nn.Module):\n",
    "    def __init__(self, alphabet):\n",
    "        super(WordInflector, self).__init__()\n",
    "        self.alphabet = alphabet.stoi\n",
    "        self.integer2char = alphabet.itos\n",
    "        alphabet_size = len(self.alphabet)\n",
    "        \n",
    "        self.encoder = Encoder(self.alphabet)\n",
    "        self.decoder = Decoder(self.alphabet)\n",
    "    \n",
    "    def get_string(self,ids):\n",
    "        string = ''.join([self.integer2char[i] for i in ids])\n",
    "        return re.sub(\"%s.*\" % END,\"\",string)\n",
    "\n",
    "    def forward(self, example):\n",
    "        encoder_hs = self.encoder(example)\n",
    "        return self.decoder(example,encoder_hs)\n",
    "            \n",
    "    def generate(self, data):\n",
    "        all_results = []\n",
    "        with torch.no_grad():\n",
    "            for example in data:\n",
    "                encoder_hs = self.encoder(example)\n",
    "                output = self.decoder.generate(encoder_hs)\n",
    "                all_results.append(self.get_string(output))\n",
    "        return all_results\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    # Read the Spanish medium data set.\n",
    "    train_iter, dev_iter, test_iter = read_data(language=\"spanish\",\n",
    "                                            setting=\"medium\",\n",
    "                                            batch_size=1)\n",
    "    \n",
    "    inflector = WordInflector(WORD.vocab)\n",
    "\n",
    "    loss_function = nn.NLLLoss(ignore_index=inflector.alphabet[PAD],reduction='mean')\n",
    "    optimizer = Adam(inflector.parameters())\n",
    "    gold_dev_words = [''.join(w.output) for w in dev_iter.dataset]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        tot_loss = 0 \n",
    "\n",
    "        # Update parameters\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            print(\"Example %u of %u\" % (i+1,len(train_iter)),end=\"\\r\")\n",
    "            inflector.zero_grad()\n",
    "            tag_scores, tgt = inflector(batch)\n",
    "            tgt = tgt.permute(1,0)\n",
    "            tag_scores = tag_scores.permute(1,2,0)\n",
    "            loss = loss_function(tag_scores,tgt) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += len(batch)*loss.detach().numpy()\n",
    "        print()\n",
    "        avg_loss = tot_loss/len(train_iter)\n",
    "        print(\"EPOCH %u: AVG LOSS PER EX: %.5f\" % (epoch+1,avg_loss))        \n",
    "\n",
    "        # Evaluate on dev data.\n",
    "        sys_dev_words = inflector.generate(dev_iter)\n",
    "        print(\"DEV ACC: %.2f%%\" % accuracy(sys_dev_words,gold_dev_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Augment the model with attention!\n",
    "\n",
    "#### Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "        def __init__(self,alphabet):\n",
    "                super(Encoder,self).__init__()\n",
    "                self.embedding = nn.Embedding(len(alphabet), EMBEDDING_DIM)\n",
    "                self.rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS, bidirectional=True)\n",
    "\n",
    "        def forward(self,ex):\n",
    "            input, _ = ex.input\n",
    "            \n",
    "            embedded = self.embedding(input)\n",
    "            # embedded = [seq_len, 1, emd_dim]\n",
    "            \n",
    "            hs, _ = self.rnn(embedded)\n",
    "            \n",
    "            return hs\n",
    "\n",
    "# An assertion to test that your implementation returns an object of the correct size. \n",
    "input, input_length = example.input\n",
    "assert(Encoder(WORD.vocab.stoi)(example).size() == torch.Size([input_length,1,2*RNN_HIDDEN_DIM]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Attention\n",
    "\n",
    "\n",
    "The `Attention` class implements a version of [Bahdanau attention](https://blog.floydhub.com/attention-mechanism/). Its `forward` function takes two inputs: a tensor of encoder hidden states `encoder_hss` of dimension `(sequence_length, 1, 2*RNN_HIDDEN_DIM)` and a decoder hidden state `dec_state` of dimension `(1,1,RNN_HIDDEN_DIM)`. It computes a context weight for each of the encoder hidden states and the decoder hidden state using a feed-forward neural network with one hidden layer and a ReLU non-linearity. These weights are then normalized into a probability distribution $p_1, ..., p_T$ using a softmax layer. Finally, `forward` will return the weighted mean $p_1 e_1 + ... + p_T e_T$.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention,self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(3*RNN_HIDDEN_DIM,RNN_HIDDEN_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(RNN_HIDDEN_DIM,1)\n",
    "    \n",
    "    def forward(self,encoder_hss,decoder_hs):\n",
    "        \n",
    "        # encoder_hss = [seq_len, 1, 2 * hid_dim]\n",
    "        # decoder_hs = [1, 1, hid_dim]\n",
    "        \n",
    "        decoder_hss = decoder_hs.expand(encoder_hss.size()[0], 1, RNN_HIDDEN_DIM)\n",
    "        \n",
    "        conditioned = torch.cat((encoder_hss, decoder_hss), dim = 2)\n",
    "        \n",
    "        # conditioned = [seq_len, 1, 3 * hid_dim]\n",
    "        \n",
    "        att = self.linear1(conditioned)\n",
    "        \n",
    "        att1 = self.relu(att)\n",
    "        \n",
    "        # att1 = [seq_len, 1, hid_dim]\n",
    "        \n",
    "        att2 = self.linear2(att1)\n",
    "        \n",
    "        # att2 = [seq_len, 1, 1]\n",
    "        att2 = nn.functional.softmax(att2, dim = 0)\n",
    "        weights = att2.expand(att2.size()[0], 1, 2 * RNN_HIDDEN_DIM)\n",
    "        \n",
    "        # weights = [seq_len, 1, 2 * RNN_HIDDEN_DIM]\n",
    "        \n",
    "        product = weights * encoder_hss\n",
    "        \n",
    "        # product = [seq_len, 1, 2 * RNN_HIDDEN_DIM]\n",
    "        \n",
    "        weighted_mean = torch.sum(product, dim = 0).unsqueeze(0)\n",
    "        # weight_mean = [1, 1, 2 * RNN_HIDDEN_DIM]\n",
    "        \n",
    "        \n",
    "        return weighted_mean\n",
    "\n",
    "# An assertion to test that your implementation returns an object of the correct size. \n",
    "input, input_length = example.input\n",
    "encoder_hss = Encoder(WORD.vocab.stoi)(example)\n",
    "decoder_hs = torch.randn(1,1,RNN_HIDDEN_DIM)\n",
    "\n",
    "assert(Attention()(encoder_hss,decoder_hs).size() == torch.Size([1,1,2*RNN_HIDDEN_DIM]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(len(alphabet), EMBEDDING_DIM)\n",
    "        self.attention = Attention()\n",
    "        self.rnn = nn.LSTM(EMBEDDING_DIM+2*RNN_HIDDEN_DIM, RNN_HIDDEN_DIM, RNN_LAYERS, bidirectional=False)\n",
    "        self.hidden2char = nn.Linear(RNN_HIDDEN_DIM, len(alphabet))\n",
    "    \n",
    "    def forward(self,ex,encoder_hss):\n",
    "        output, output_length = ex.output\n",
    "        embedded_output = self.embedding(output[:-1])\n",
    "        # embedded_output = [output_length - 1,1,EMBEDDING_DIM]\n",
    "        \n",
    "        results = []\n",
    "        decoder_state = (torch.zeros(1,1,RNN_HIDDEN_DIM,requires_grad=False), \n",
    "                         torch.zeros(1,1,RNN_HIDDEN_DIM,requires_grad=False))\n",
    "        \n",
    "            \n",
    "        for i in range(output_length - 1):\n",
    "\n",
    "            context = self.attention(encoder_hss, decoder_state[0])\n",
    "            # context = [1,1,2*RNN_HIDDEN_DIM]\n",
    "\n",
    "            cat = torch.cat((embedded_output[i, :, :].unsqueeze(0), context), dim = 2)\n",
    "\n",
    "            _, decoder_state = self.rnn(cat, decoder_state)\n",
    "\n",
    "            hs = decoder_state[0]\n",
    "\n",
    "            hid2char = self.hidden2char(hs)\n",
    "            \n",
    "            hid2char = nn.functional.log_softmax(hid2char, dim = 2)\n",
    "\n",
    "            results.append(hid2char)\n",
    "                \n",
    "        \n",
    "        out = torch.cat(results, dim = 0)\n",
    "        # out = [seq_len - 1, 1, alphabet_size]\n",
    "        \n",
    "        return out, output[1:]\n",
    "\n",
    "    def generate(self,encoder_hss):\n",
    "        with torch.no_grad():\n",
    "            decoder_state = (torch.zeros(1,1,RNN_HIDDEN_DIM), torch.zeros(1,1,RNN_HIDDEN_DIM))\n",
    "            output_char = torch.LongTensor([[self.alphabet[START]]])\n",
    "            result = []\n",
    "            for _ in range(MAXWFLEN):\n",
    "                \n",
    "                output_embedding = self.embedding(output_char)\n",
    "                \n",
    "                # output_embedding = [1, 1, emb_dim]\n",
    "                \n",
    "                context = self.attention(encoder_hss, decoder_state[0])\n",
    "                \n",
    "                # context = [1, 1, 2 * hid_dim]\n",
    "                \n",
    "                cat = torch.cat((output_embedding, context), dim = 2)\n",
    "                \n",
    "                # cat = [1, 1, emd_dim + 2 * hid_dim]\n",
    "                \n",
    "                _, decoder_state = self.rnn(cat, decoder_state)\n",
    "                \n",
    "                hs = decoder_state[0]\n",
    "                \n",
    "                # hs = [1, 1, hid_dim]\n",
    "                \n",
    "                hid2char = self.hidden2char(hs)\n",
    "                \n",
    "                # hid2char = [1, 1, len(alphabet)]\n",
    "                \n",
    "                hid2char = nn.functional.log_softmax(hid2char, dim = 2)\n",
    "                \n",
    "                output_char = torch.argmax(hid2char, dim = 2)\n",
    "                               \n",
    "                result.append(output_char.numpy().tolist()[0][0])\n",
    "                \n",
    "        return result\n",
    "            \n",
    "# Assertions to test the implementation returns objects of the correct size. \n",
    "encoder_hs = Encoder(WORD.vocab.stoi)(example)\n",
    "_, output_length = example.output\n",
    "alphabet = WORD.vocab.stoi\n",
    "alphabet_size = len(alphabet)\n",
    "assert(Decoder(alphabet)(example,encoder_hs)[0].size() == torch.Size([output_length - 1,1,alphabet_size]))\n",
    "assert(len(Decoder(alphabet).generate(encoder_hs)) == MAXWFLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 999 of 999\n",
      "EPOCH 1: AVG LOSS PER EX: 2.44053\n",
      "DEV ACC: 0.00%\n",
      "Example 999 of 999\n",
      "EPOCH 2: AVG LOSS PER EX: 1.43504\n",
      "DEV ACC: 1.80%\n",
      "Example 999 of 999\n",
      "EPOCH 3: AVG LOSS PER EX: 0.83666\n",
      "DEV ACC: 18.32%\n",
      "Example 999 of 999\n",
      "EPOCH 4: AVG LOSS PER EX: 0.52703\n",
      "DEV ACC: 43.34%\n",
      "Example 999 of 999\n",
      "EPOCH 5: AVG LOSS PER EX: 0.38147\n",
      "DEV ACC: 47.85%\n",
      "Example 999 of 999\n",
      "EPOCH 6: AVG LOSS PER EX: 0.31022\n",
      "DEV ACC: 41.34%\n",
      "Example 999 of 999\n",
      "EPOCH 7: AVG LOSS PER EX: 0.25495\n",
      "DEV ACC: 39.84%\n",
      "Example 999 of 999\n",
      "EPOCH 8: AVG LOSS PER EX: 0.21282\n",
      "DEV ACC: 63.16%\n",
      "Example 999 of 999\n",
      "EPOCH 9: AVG LOSS PER EX: 0.17004\n",
      "DEV ACC: 51.55%\n",
      "Example 999 of 999\n",
      "EPOCH 10: AVG LOSS PER EX: 0.15390\n",
      "DEV ACC: 69.37%\n"
     ]
    }
   ],
   "source": [
    "class WordInflector(nn.Module):\n",
    "    def __init__(self, alphabet):\n",
    "        super(WordInflector, self).__init__()\n",
    "        self.c2i = alphabet.stoi\n",
    "        self.i2c = alphabet.itos\n",
    "        alphabet_size = len(self.c2i)\n",
    "        \n",
    "        self.encoder = Encoder(self.c2i)\n",
    "        self.decoder = Decoder(self.c2i)\n",
    "    \n",
    "    def get_string(self,ids):\n",
    "        string = ''.join([self.i2c[i] for i in ids])\n",
    "        return re.sub(\"%s.*\" % END,\"\",string)\n",
    "\n",
    "    def forward(self, example):\n",
    "        encoder_hs = self.encoder(example)\n",
    "        return self.decoder(example,encoder_hs)\n",
    "            \n",
    "    def generate(self, data):\n",
    "        all_results = []\n",
    "        with torch.no_grad():\n",
    "            for example in data:\n",
    "                encoder_hs = self.encoder(example)\n",
    "                output = self.decoder.generate(encoder_hs)\n",
    "                all_results.append(self.get_string(output))\n",
    "        return all_results\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    train_iter, dev_iter, test_iter = read_data(language=\"spanish\",\n",
    "                                            setting=\"medium\",\n",
    "                                            batch_size=1)\n",
    "    \n",
    "    inflector = WordInflector(WORD.vocab)\n",
    "\n",
    "    loss_function = nn.NLLLoss(ignore_index=inflector.c2i[PAD],reduction='mean')\n",
    "    optimizer = Adam(inflector.parameters())\n",
    "    gold_dev_words = [''.join(w.output) for w in dev_iter.dataset]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        tot_loss = 0 \n",
    "\n",
    "        # Update parameters\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            print(\"Example %u of %u\" % (i+1,len(train_iter)),end=\"\\r\")\n",
    "            inflector.zero_grad()\n",
    "            tag_scores, tgt = inflector(batch)\n",
    "            tgt = tgt.permute(1,0)\n",
    "            tag_scores = tag_scores.permute(1,2,0)\n",
    "            loss = loss_function(tag_scores,tgt) \n",
    "            tot_loss += loss.detach().numpy()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print()\n",
    "        avg_loss = tot_loss/len(train_iter)\n",
    "        print(\"EPOCH %u: AVG LOSS PER EX: %.5f\" % (epoch+1,avg_loss))        \n",
    "\n",
    "        # Evaluate on dev data.\n",
    "        sys_dev_words = inflector.generate(dev_iter)\n",
    "        print(\"DEV ACC: %.2f%%\" % accuracy(sys_dev_words,gold_dev_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
