{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VdDpixSte1yy"
   },
   "source": [
    "# Paraphrase generation of Portugese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "- torchtext\n",
    "- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "De5_yaam200J",
    "outputId": "049b9d5f-736d-41a0-ecef-b9c782b5bf9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# required libraries\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PSVn4bw-3D5-"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "wK6scNsy3FxR",
    "outputId": "5e6519bf-4a80-4c21-9a64-2f60d362b581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
      "\u001b[K     |████████████████████████████████| 21.2MB 42.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.21.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (46.0.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.38.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: pt-core-news-sm\n",
      "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp36-none-any.whl size=21186282 sha256=f097afc6be44626dfef27a88104cb8ed371969bfa59710798fc56976348bab9c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8moqg9t0/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
      "Successfully built pt-core-news-sm\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n",
      "Number of training examples: 85171\n",
      "Number of validation examples: 4724\n",
      "Number of testing examples: 4692\n",
      "Unique tokens in source vocabulary: 7944\n",
      "Unique tokens in target vocabulary: 3052\n",
      "training batch\n",
      "tensor size of source language: torch.Size([7, 16])\n",
      "tensor size of target language: torch.Size([9, 16])\n",
      "validation batch\n",
      "tensor size of source language: torch.Size([9, 256])\n",
      "tensor size of target language: torch.Size([11, 256])\n",
      "(blind) test batch\n",
      "tensor size of source language: torch.Size([15, 256])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "tokenization code\n",
    "'''\n",
    "\n",
    "!python -m spacy download pt_core_news_sm\n",
    "\n",
    "import pt_core_news_sm\n",
    "\n",
    "spacy_pt = pt_core_news_sm.load() # just need tokenizer from one language as both source and target are from the same language\n",
    "\n",
    "def tokenize_pt(text):\n",
    "    \"\"\"\n",
    "    Tokenizes Portuguese text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_pt.tokenizer(text)]\n",
    "\n",
    "'''\n",
    "define field\n",
    "'''\n",
    "SRC = torchtext.data.Field(tokenize = tokenize_pt, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TRG = torchtext.data.Field(tokenize = tokenize_pt, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "'''\n",
    "load the data\n",
    "'''\n",
    "train_data = torchtext.data.TabularDataset(\n",
    "    path='/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp-train.tsv', \n",
    "    format='tsv', skip_header=True, fields=[('SRC', SRC), ('TRG', TRG)])\n",
    "valid_data = torchtext.data.TabularDataset(\n",
    "    path='/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp-valid.tsv', \n",
    "    format='tsv', skip_header=True, fields=[('SRC', SRC), ('TRG', TRG)])\n",
    "test_data = torchtext.data.TabularDataset(\n",
    "    path='/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp-test-onlySRC.tsv', \n",
    "    format='tsv', skip_header=True, fields=[('SRC', SRC)]) # blind test data (that is, no targets)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "'''\n",
    "build the vocabulary\n",
    "'''\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "print(f\"Unique tokens in source vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target vocabulary: {len(TRG.vocab)}\")\n",
    "\n",
    "'''\n",
    "create the iterator\n",
    "'''\n",
    "train_iter = torchtext.data.BucketIterator(train_data, batch_size=16, device=device, sort_key=lambda x: len(x.SRC), sort_within_batch=True)\n",
    "valid_iter = torchtext.data.BucketIterator(valid_data, batch_size=256, device=device, sort_key=lambda x: len(x.SRC), sort_within_batch=True)\n",
    "test_iter = torchtext.data.Iterator(test_data, batch_size=256, device=device, sort=False, sort_key=None, shuffle=False, sort_within_batch=False)\n",
    "\n",
    "'''\n",
    "print sample batch\n",
    "'''\n",
    "# print first batch of training data\n",
    "print('training batch')\n",
    "for batch in train_iter:\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    print('tensor size of target language:', trg.shape)\n",
    "    break\n",
    "\n",
    "# print first batch of validation data\n",
    "print('validation batch')\n",
    "for batch in valid_iter:\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    print('tensor size of target language:', trg.shape)\n",
    "    break\n",
    "\n",
    "# print first batch of test data\n",
    "print('(blind) test batch')\n",
    "for batch in test_iter:\n",
    "    src = batch.SRC\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    break\n",
    "\n",
    "# save the field\n",
    "import pickle\n",
    "with open(\"./drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/TRG.Field\",\"wb\")as f:\n",
    "     pickle.dump(TRG,f)\n",
    "\n",
    "with open(\"./drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/SRC.Field\",\"wb\")as f:\n",
    "     pickle.dump(SRC,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKmz-QHv629g"
   },
   "outputs": [],
   "source": [
    "# Alternate pre-trained models\n",
    "\n",
    "# load pre-trained embeddings\n",
    "\n",
    "ptwiki_trg = \"/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp_ptwiki_trg_vec.pt\"\n",
    "ptwiki_src = \"/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp_ptwiki_src_vec.pt\"\n",
    "\n",
    "fastt_trg = \"/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp_fastt_trg_vec.pt\"\n",
    "fastt_src = \"/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp_fastt_src_vec.pt\"\n",
    "\n",
    "\n",
    "trg_vec = torch.load(fastt_trg)\n",
    "src_vec = torch.load(fastt_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZGoARLtC9QV"
   },
   "source": [
    "## Attention Seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WpPua3bKDWU2"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout, bidirection):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)#.from_pretrained(src_vec, freeze = False)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout, bidirectional = bidirection)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, dec_hidden, encoder_outputs):\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * num_directions]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len-1 times\n",
    "        hidden = dec_hidden.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 2, 0)\n",
    "        \n",
    "        #hidden = [batch size, 1, dec hid dim]\n",
    "        #encoder_outputs = [batch size, enc hid dim * num_directions, src_len]\n",
    "        \n",
    "        # attention scoring function : S dot H_encoder\n",
    "        attention = torch.bmm(hidden, encoder_outputs).squeeze(1)\n",
    "   \n",
    "        # attention = [batch size, src len]\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)#.from_pretrained(trg_vec, freeze = False)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc_mid = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size], current token\n",
    "        #hidden = [batch size, dec hid dim], previous decoder hidden state\n",
    "        #cell = [batch size, dec hid dim], previous decoder cell state\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * num_directions], all encoder hidden states (H)\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        # Now, get the current hidden state of the decoder\n",
    "\n",
    "        dec_output, (dec_hidden, dec_cell) = self.rnn(embedded, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        dec_hidden = dec_hidden.squeeze(0)\n",
    "        # dec_hidden = [batch size, dec hid dim]\n",
    "\n",
    "        dec_cell = dec_cell.squeeze(0)\n",
    "        # dec_cell = [batch size, dec hid dim]\n",
    "\n",
    "        # Use the current hidden state of the decoder, get the attention probabilities\n",
    "\n",
    "        attention_weights = self.attention(dec_hidden, encoder_outputs)\n",
    "        # attention weights = [batch size, src_len]\n",
    "\n",
    "        ## We need to create weighted context vector\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        # attention weights = [batch size, 1, src_len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # encoder_outputs = [batch size, src len, enc hid dim * num_directions]\n",
    "        # perform weighted sum of encoder hidden states to get attention output\n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * num_directions]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * num_directions]\n",
    "\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        # weighted = [batch size, enc hid dim * num_directions]\n",
    "\n",
    "        ## concatenate the weighted context vector with the current decoder hidden state\n",
    "\n",
    "        conc = torch.cat((weighted, dec_hidden), dim = 1)\n",
    "\n",
    "        # conc = [batch size, enc hid dim * num_directions + dec hid dim]\n",
    "\n",
    "        ## Pass through the f_mid Linear layer to get S'\n",
    "\n",
    "        S_new = self.fc_mid(conc)\n",
    "\n",
    "        # S_new = [batch size, dec hid dim]\n",
    "\n",
    "        ## Then pass the f_out Linear layer to get predictions\n",
    "\n",
    "        prediction = self.fc_out(S_new)\n",
    "\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction, dec_hidden, dec_cell, attention_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # save the encoder-decoder attention weights\n",
    "        # all_attention_weights = [batch_size, trg len-1, src len ]\n",
    "        all_attention_weights = torch.zeros(trg.shape[1], trg.shape[0]-1, src.shape[0])\n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "          \n",
    "        hidden = torch.mean(encoder_outputs, dim=0)\n",
    "        cell = torch.cat((cell[0, :, :].unsqueeze(0), cell[1, :, :].unsqueeze(0)), dim = 2).squeeze(0)\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, cell, attention_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # all_attention_weights[t-1] = [src len, batch size]\n",
    "            all_attention_weights[:,t-1,:] = attention_weights.squeeze(1)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs,all_attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lziYorLsW40n"
   },
   "outputs": [],
   "source": [
    "def inference(model, file_name, src_vocab, trg_vocab, attention= True, batch_size = 128, max_trg_len = 64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from torchtext.data import TabularDataset\n",
    "    from torchtext.data import Iterator\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "\n",
    "    test = TabularDataset(\n",
    "      path=file_name, # the root directory where the data lies\n",
    "      format='tsv',\n",
    "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "      fields=[('SRC', src_vocab), ('TRG', trg_vocab)])\n",
    "\n",
    "    test_iter = Iterator(\n",
    "    dataset = test, # we pass in the datasets we want the iterator to draw data from\n",
    "    sort = False,batch_size=batch_size,\n",
    "    sort_key=None,\n",
    "    shuffle=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device,\n",
    "    train=False\n",
    "    )\n",
    "  \n",
    "    model.eval()\n",
    "    all_trg = []\n",
    "    all_translated_trg = []\n",
    "\n",
    "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_iter):\n",
    "\n",
    "            src = batch.SRC\n",
    "            #src = [src len, batch size]\n",
    "\n",
    "            trg = batch.TRG\n",
    "            #trg = [trg len, batch size]\n",
    "\n",
    "            batch_size = trg.shape[1]\n",
    "\n",
    "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "            trg_placeholder = trg_placeholder.long().to(device)\n",
    "            if attention == True:\n",
    "              output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            else:\n",
    "              output = model(src, trg_placeholder) #turn off teacher forcing\n",
    "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
    "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "            output_translate = output[1:]\n",
    "            # store gold target sentences to a list \n",
    "            all_trg.append(trg[1:].cpu())\n",
    "\n",
    "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "            prob, token_id = output_translate.data.topk(1)\n",
    "            translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "            # store gold target sentences to a list \n",
    "            all_translated_trg.append(translation_token_id)\n",
    "      \n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    for i in range(len(all_trg)): \n",
    "        cur_gold = all_trg[i]\n",
    "        cur_translation = all_translated_trg[i]\n",
    "        for j in range(cur_gold.shape[1]):\n",
    "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:,j])\n",
    "            gold_convered_strings = [c.lower() for c in gold_convered_strings]\n",
    "\n",
    "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
    "            trans_convered_strings = [c.lower() for c in trans_convered_strings]\n",
    "\n",
    "            all_gold_text.append(gold_convered_strings)\n",
    "            all_translated_text.append(trans_convered_strings)\n",
    "\n",
    "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
    "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
    "    return corpus_bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "coD24BprDszs"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "id": "x6k65PC7DuuD",
    "outputId": "f9b3d79a-537d-4eb1-fb50-0e6793f10417"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> token index:  1\n",
      "The model has 16,446,444 trainable parameters\n",
      "Epoch: 01 | Time: 6m 39s\n",
      "\t Train Loss: 1.761 | Train PPL:   5.820\n",
      "\t Val. Loss: 5.134 |  Val. PPL: 169.752\n",
      "\t Train BLEU:0.802 |  Val. BLEU:   0.230\n",
      "Epoch: 02 | Time: 6m 43s\n",
      "\t Train Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 6.548 |  Val. PPL: 697.534\n",
      "\t Train BLEU:0.928 |  Val. BLEU:   0.239\n",
      "Epoch: 03 | Time: 6m 50s\n",
      "\t Train Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 8.199 |  Val. PPL: 3638.703\n",
      "\t Train BLEU:0.960 |  Val. BLEU:   0.232\n",
      "Epoch: 04 | Time: 6m 51s\n",
      "\t Train Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 8.744 |  Val. PPL: 6273.482\n",
      "\t Train BLEU:0.969 |  Val. BLEU:   0.247\n",
      "Epoch: 05 | Time: 6m 48s\n",
      "\t Train Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 9.880 |  Val. PPL: 19533.863\n",
      "\t Train BLEU:0.979 |  Val. BLEU:   0.250\n",
      "Epoch: 06 | Time: 6m 47s\n",
      "\t Train Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 10.963 |  Val. PPL: 57699.342\n",
      "\t Train BLEU:0.983 |  Val. BLEU:   0.251\n",
      "Epoch: 07 | Time: 6m 50s\n",
      "\t Train Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 11.517 |  Val. PPL: 100371.556\n",
      "\t Train BLEU:0.987 |  Val. BLEU:   0.253\n",
      "Epoch: 08 | Time: 6m 49s\n",
      "\t Train Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 12.891 |  Val. PPL: 396648.110\n",
      "\t Train BLEU:0.988 |  Val. BLEU:   0.244\n",
      "Epoch: 09 | Time: 6m 53s\n",
      "\t Train Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 13.856 |  Val. PPL: 1041644.734\n",
      "\t Train BLEU:0.989 |  Val. BLEU:   0.242\n",
      "Epoch: 10 | Time: 6m 46s\n",
      "\t Train Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 14.230 |  Val. PPL: 1514024.173\n",
      "\t Train BLEU:0.914 |  Val. BLEU:   0.181\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "hyperparameters\n",
    "'''\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "N_LAYERS = 1\n",
    "CLIP = 1\n",
    "BI_DIRECTION = True\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 2 * ENC_HID_DIM #(you should figure out this hyper-parameter). \n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.8\n",
    "TEACH_FORCING_RATE = 0.7\n",
    "LEARNING_RT = 0.001\n",
    "WEIGHT_DECAY = 0.0000\n",
    "MAX_EPOCH = 10\n",
    "\n",
    "train_batch_size = 16\n",
    "val_batch_size = 256\n",
    "'''\n",
    "instantiate the model\n",
    "'''\n",
    "attn = Attention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT, BI_DIRECTION)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RT, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "print('<pad> token index: ',TRG_PAD_IDX)\n",
    "## we will ignore the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "'''\n",
    "initialize the model weights\n",
    "'''\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "'''\n",
    "calculate the number of parameters\n",
    "'''\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# convert index to text string\n",
    "def convert_itos(convert_vocab, token_ids):\n",
    "    list_string = []\n",
    "    for i in token_ids:\n",
    "        if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "            break\n",
    "        else:\n",
    "            token = convert_vocab.vocab.itos[i]\n",
    "            list_string.append(token)\n",
    "    return list_string\n",
    "\n",
    "\n",
    "'''\n",
    "Full training helper functions\n",
    "'''\n",
    "\n",
    "def train_attn(model, iterator, optimizer, criterion, clip):\n",
    "    manual_seed = 77\n",
    "    torch.manual_seed(manual_seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed(manual_seed)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.SRC\n",
    "        trg = batch.TRG\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output,_ = model(src, trg, teacher_forcing_ratio = TEACH_FORCING_RATE)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    bleu = inference(model, \"/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp-train.tsv\", SRC, TRG, True, train_batch_size, 64)\n",
    "    return epoch_loss / len(iterator), bleu\n",
    "\n",
    "def evaluate_attn(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.SRC\n",
    "            trg = batch.TRG\n",
    "\n",
    "            output,_ = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    bleu = inference(model, \"/content/drive/My Drive/Colab Notebooks/lab4/data/paraphrase_generation/pp-valid.tsv\", SRC, TRG, True, val_batch_size, 64)\n",
    "    return epoch_loss / len(iterator), bleu\n",
    "\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "'''\n",
    "kickstart full training\n",
    "'''\n",
    "\n",
    "manual_seed = 77\n",
    "torch.manual_seed(manual_seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss,bleu_train = train_attn(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, bleu = evaluate_attn(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Create checkpoint at end of each epoch\n",
    "    state_dict_model = model.state_dict() \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': state_dict_model,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    torch.save(state, \"/content/drive/My Drive/Colab Notebooks/lab4/ckpt/pp_\"+str(epoch+1)+\".pt\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(f'\\t Train BLEU:{bleu_train:.3f} |  Val. BLEU: {bleu:7.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEHRD0kVQb4b"
   },
   "source": [
    "## Preparing Predictions from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_h6fUiSCDuEz",
    "outputId": "859574f2-13f1-41ee-9558-0e635c2e9b61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['há muitas estudantes tinha completar food .', 'por que você está escrito para o lugar ?', 'onde você fez nossos bagagem para a estudante ?', 'o rei comprou uma copo na caixa .', 'eu preciso abrir a porta .', 'o cavalo está mudar de verdade !', 'meu irmão sempre queria um médico .', 'há muitas pintores n esta trem , não há ?', 'é uma casa estas últimos uma pergunta .', 'onde está a estação de metrô é aqui ?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load fields saved during preprocessing\n",
    "'''\n",
    "# with open(\"./drive/My Drive/Colab Notebooks/ckpt_pp_lab4/TRG.Field\",\"rb\") as f:\n",
    "#      TRG_saved = pickle.load(f)\n",
    "\n",
    "# with open(\"./drive/My Drive/Colab Notebooks/ckpt_pp_lab4/SRC.Field\",\"rb\") as f:\n",
    "#      SRC_saved = pickle.load(f)\n",
    "\n",
    "TRG_saved = TRG\n",
    "SRC_saved = SRC\n",
    "\n",
    "'''\n",
    "instantiate the model\n",
    "'''\n",
    "attn = Attention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT, BI_DIRECTION)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model_best = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "'''\n",
    "load the checkpoint corresponding to the best epoch (usually epoch with highest validation BLEU score)\n",
    "'''\n",
    "model_best.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/lab4/ckpt/pp_7.pt')['state_dict'])\n",
    "model_best = model_best.to(device)\n",
    "\n",
    "'''\n",
    "generate paraphrases for all the sentences in test data\n",
    "'''\n",
    "\n",
    "def translation_inference(token_id):\n",
    "  sent = []\n",
    "  for i in token_id:\n",
    "    if i == TRG.vocab.stoi[\"<eos>\"]:\n",
    "      break\n",
    "    else:\n",
    "      token = TRG.vocab.itos[i]\n",
    "      sent.append(token)\n",
    "  \n",
    "  return sent\n",
    "\n",
    "def generate_paraphrases(model, eval_iter, trg_vocab, attention = True, max_trg_len = 64):\n",
    "  '''\n",
    "    Function for generating paraphrases by model inference\n",
    "\n",
    "    Input: \n",
    "    model: paraphrase generation model;\n",
    "    eval_iter: iterator over the evaluation data\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of paraphrase text (optional), default = 64\n",
    "\n",
    "    Output:\n",
    "    List of predicted paraphrases\n",
    "  '''\n",
    "  model.eval()\n",
    "  all_translation_word_ids = []\n",
    "  all_gold_sents = []\n",
    "  for batch in eval_iter:\n",
    "    src = batch.SRC\n",
    "    #src = [src len, batch size]\n",
    "    batch_size = src.shape[1]\n",
    "\n",
    "    # gold = batch.TRG\n",
    "\n",
    "    # for i in range(src.shape[1]):\n",
    "    #   gold_id = gold[1:, i].cpu().numpy()\n",
    "    #   gold_sent = translation_inference(gold_id)\n",
    "    #   all_gold_sents.append(gold_sent)\n",
    "\n",
    "\n",
    "    # create a placeholder for target language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "    trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "    trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "    trg_placeholder = trg_placeholder.long().to(device)\n",
    "    if attention == True:\n",
    "      output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "    else:\n",
    "      output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "    # get translation results, we ignore first token <sos> in both translation and target sentences. \n",
    "    # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "    output_translate = output[1:]\n",
    "\n",
    "    # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "    prob, token_id = output_translate.data.topk(1)\n",
    "    translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "    # store gold target sentences to a list \n",
    "    all_translation_word_ids.append(translation_token_id)\n",
    "  \n",
    "  all_translation_text = []\n",
    "  for i in range(len(all_translation_word_ids)):\n",
    "    cur_translation_batch = all_translation_word_ids[i]\n",
    "    for j in range(cur_translation_batch.shape[1]):\n",
    "      trans_convered_strings = convert_itos(trg_vocab, cur_translation_batch[:,j])\n",
    "      all_translation_text.append(' '.join(trans_convered_strings)) # convert list of words to text\n",
    "  \n",
    "  return all_translation_text #, all_gold_sents\n",
    "\n",
    "# translate all the sentences in the test set      \n",
    "test_predictions = generate_paraphrases(model, test_iter, TRG_saved, attention = True, max_trg_len = 64)\n",
    "print(test_predictions[:10])\n",
    "#print(gold[:10])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "paraphrase_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
