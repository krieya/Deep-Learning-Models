{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqDvmzPtY3j_"
   },
   "source": [
    "# Machine Translation Task (Portuguese to English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDsWzitHZO3x"
   },
   "source": [
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "- torchtext\n",
    "- NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7vLl5mFKCgO"
   },
   "source": [
    "## Import libraries, dataset and prepare necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n76MDk3BZSPI"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKmTctbOqD1S"
   },
   "outputs": [],
   "source": [
    "def inference(model, file_name, src_vocab, trg_vocab, attention= True, batch_size = 128, max_trg_len = 64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from torchtext.data import TabularDataset\n",
    "    from torchtext.data import Iterator\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "\n",
    "    test = TabularDataset(\n",
    "      path=file_name, # the root directory where the data lies\n",
    "      format='tsv',\n",
    "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "      fields=[('SRC', src_vocab), ('TRG', trg_vocab)])\n",
    "\n",
    "    test_iter = Iterator(\n",
    "    dataset = test, # we pass in the datasets we want the iterator to draw data from\n",
    "    sort = False,batch_size=batch_size,\n",
    "    sort_key=None,\n",
    "    shuffle=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device,\n",
    "    train=False\n",
    "    )\n",
    "  \n",
    "    model.eval()\n",
    "    all_trg = []\n",
    "    all_translated_trg = []\n",
    "\n",
    "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_iter):\n",
    "\n",
    "            src = batch.SRC\n",
    "            #src = [src len, batch size]\n",
    "\n",
    "            trg = batch.TRG\n",
    "            #trg = [trg len, batch size]\n",
    "\n",
    "            batch_size = trg.shape[1]\n",
    "\n",
    "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "            trg_placeholder = trg_placeholder.long().to(device)\n",
    "            if attention == True:\n",
    "              output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            else:\n",
    "              output = model(src, trg_placeholder) #turn off teacher forcing\n",
    "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
    "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "            output_translate = output[1:]\n",
    "            # store gold target sentences to a list \n",
    "            all_trg.append(trg[1:].cpu())\n",
    "\n",
    "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "            prob, token_id = output_translate.data.topk(1)\n",
    "            translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "            # store gold target sentences to a list \n",
    "            all_translated_trg.append(translation_token_id)\n",
    "      \n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    for i in range(len(all_trg)): \n",
    "        cur_gold = all_trg[i]\n",
    "        cur_translation = all_translated_trg[i]\n",
    "        for j in range(cur_gold.shape[1]):\n",
    "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:,j])\n",
    "            gold_convered_strings = [c.lower() for c in gold_convered_strings]\n",
    "\n",
    "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
    "            trans_convered_strings = [c.lower() for c in trans_convered_strings]\n",
    "\n",
    "            all_gold_text.append(gold_convered_strings)\n",
    "            all_translated_text.append(trans_convered_strings)\n",
    "\n",
    "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
    "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
    "    return corpus_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4jZHt5PhSKT8",
    "outputId": "311a8763-9e0e-462f-8442-edd2d85bd991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "manual_seed = 77\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "id": "a3JKxrSHZpND",
    "outputId": "9211f3cc-e09a-4e81-ea73-113ee6ed3065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Collecting pt_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
      "\u001b[K     |████████████████████████████████| 21.2MB 5.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.21.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (46.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.38.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: pt-core-news-sm\n",
      "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp36-none-any.whl size=21186282 sha256=1044924428db1344af734311a64b606590712673b0dfd8a57a1a22f88b9e5e25\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sgqkdzno/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
      "Successfully built pt-core-news-sm\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_M9buV5UZsNF"
   },
   "outputs": [],
   "source": [
    "import pt_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "spacy_pt = pt_core_news_sm.load()\n",
    "spacy_en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUwbI4kCZ0WF"
   },
   "outputs": [],
   "source": [
    "def tokenize_pt(text):\n",
    "    \"\"\"\n",
    "    Tokenizes PT text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_pt.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJH_gzd_aDp4"
   },
   "outputs": [],
   "source": [
    "SRC = torchtext.data.Field(tokenize = tokenize_pt, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TRG = torchtext.data.Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fO4LlPVaFWp"
   },
   "outputs": [],
   "source": [
    "train, val= torchtext.data.TabularDataset.splits(\n",
    "    path='./drive/My Drive/Colab Notebooks/lab4/data/machine_translation', train='mt-train-pt2en.tsv',validation='mt-valid-pt2en.tsv', \n",
    "    format='tsv', skip_header=True, fields=[('SRC', SRC), ('TRG', TRG)])\n",
    "test = torchtext.data.TabularDataset(\n",
    "    path='./drive/My Drive/Colab Notebooks/lab4/data/machine_translation/mt-test-pt2en_onlySRCpt.tsv',\n",
    "    format='tsv', skip_header=True, fields=[('SRC', SRC)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "v6htV_BTaKHH",
    "outputId": "eb107eea-326f-45f8-d1de-8039e05fab5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 85918\n",
      "Number of validation examples: 4772\n",
      "Number of testing examples: 4738\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VX-HY4mX0L4X"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./drive/My Drive/Colab Notebooks/lab4/SRC.field\",\"rb\")as f:\n",
    "     pt_vec = pickle.load(f).vocab.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S3kDxAlgydw"
   },
   "outputs": [],
   "source": [
    "# # try pre-trained embeddings for initialization\n",
    "\n",
    "fasttext_pt_path = \"/content/drive/My Drive/Colab Notebooks/lab4/pretrained/fasttext_pt_word2vec.pt\"\n",
    "glove_en_path = \"/content/drive/My Drive/Colab Notebooks/lab4/pretrained/glove_word2vec.pt\"\n",
    "google_en_path = \"/content/drive/My Drive/Colab Notebooks/lab4/pretrained/google_word2vec.pt\"\n",
    "\n",
    "#src_emb = torch.load(fasttext_pt_path)\n",
    "trg_emb = torch.load(glove_en_path)\n",
    "\n",
    "src_emb = torch.load(fasttext_pt_path)\n",
    "#src_emb = pt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEb54k2RbbVj"
   },
   "outputs": [],
   "source": [
    "TRG.build_vocab(train, min_freq = 2)\n",
    "SRC.build_vocab(train, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "n_9IkYcFbh7Q",
    "outputId": "8c110b94-508f-4c2f-c921-0e155ba81702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (pt) vocabulary: 7891\n",
      "Unique tokens in target (en) vocabulary: 2320\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (pt) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uHDegs7DbjJc",
    "outputId": "ade4dfdb-3b50-4808-c425-a11ca80460da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DT0ao7CbmlY"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "val_batch_size = 256\n",
    "\n",
    "train_iter, val_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val), # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_sizes=(train_batch_size, val_batch_size),device = device,\n",
    "    sort_key=lambda x: len(x.SRC), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False)\n",
    "\n",
    "\n",
    "test_iter = torchtext.data.Iterator(test,\n",
    "    sort = False, batch_size=val_batch_size,\n",
    "    sort_key=None,\n",
    "    shuffle=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device,\n",
    "    train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "_RjwK8G8bnwa",
    "outputId": "1bc53590-56ee-42e8-dc9d-73c1dc78a68e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size of source language: torch.Size([14, 16])\n",
      "tensor size of target language: torch.Size([15, 16])\n",
      "the tensor of first example in target language: tensor([  2,  27,  10,   8, 116, 308,   4,   3,   1,   1,   1,   1,   1,   1,\n",
      "          1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# batch example of training data\n",
    "for batch in train_iter:\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    print('tensor size of target language:', trg.shape)\n",
    "    print('the tensor of first example in target language:', trg[:,0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfIRzs9jDnrJ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/lab4/data/machine_translation/TRG.Field\",\"wb\")as f:\n",
    "     pickle.dump(TRG,f)\n",
    "\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/lab4/data/machine_translation/SRC.Field\", \"wb\")as f:\n",
    "     pickle.dump(SRC,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xLGko1hNQVG"
   },
   "source": [
    "# Try with seq2seq Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Lwuit7cNiDX"
   },
   "outputs": [],
   "source": [
    "def train_attn(model, iterator, optimizer, criterion, clip):\n",
    "    manual_seed = 77\n",
    "    torch.manual_seed(manual_seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed(manual_seed)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.SRC\n",
    "        trg = batch.TRG\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output,_ = model(src, trg, teacher_forcing_ratio = TEACH_FORCING_RATE)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    bleu = inference(model, \"/content/drive/My Drive/Colab Notebooks/lab4/data/machine_translation/mt-train-pt2en.tsv\", SRC, TRG, True, train_batch_size, 64)\n",
    "    return epoch_loss / len(iterator), bleu\n",
    "\n",
    "def evaluate_attn(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.SRC\n",
    "            trg = batch.TRG\n",
    "\n",
    "            output,_ = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    bleu = inference(model, \"/content/drive/My Drive/Colab Notebooks/lab4/data/machine_translation/mt-valid-pt2en.tsv\", SRC, TRG, True, val_batch_size, 64)\n",
    "    return epoch_loss / len(iterator), bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCp-mO97NsRH"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout, bidirection):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)#.from_pretrained(src_emb, freeze = False)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout, bidirectional = bidirection)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXQg_bhZNyBr"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, dec_hidden, encoder_outputs):\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * num_directions]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len-1 times\n",
    "        hidden = dec_hidden.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 2, 0)\n",
    "        \n",
    "        #hidden = [batch size, 1, dec hid dim]\n",
    "        #encoder_outputs = [batch size, enc hid dim * num_directions, src_len]\n",
    "        \n",
    "        # attention scoring function : S dot H_encoder\n",
    "        attention = torch.bmm(hidden, encoder_outputs).squeeze(1)\n",
    "   \n",
    "        # attention = [batch size, src len]\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiN9D4gKN0XA"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)#.from_pretrained(trg_emb, freeze = False)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc_mid = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size], current token\n",
    "        #hidden = [batch size, dec hid dim], previous decoder hidden state\n",
    "        #cell = [batch size, dec hid dim], previous decoder cell state\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * num_directions], all encoder hidden states (H)\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        # Now, get the current hidden state of the decoder\n",
    "\n",
    "        dec_output, (dec_hidden, dec_cell) = self.rnn(embedded, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        dec_hidden = dec_hidden.squeeze(0)\n",
    "        # dec_hidden = [batch size, dec hid dim]\n",
    "\n",
    "        dec_cell = dec_cell.squeeze(0)\n",
    "        # dec_cell = [batch size, dec hid dim]\n",
    "\n",
    "        # Use the current hidden state of the decoder, get the attention probabilities\n",
    "\n",
    "        attention_weights = self.attention(dec_hidden, encoder_outputs)\n",
    "        # attention weights = [batch size, src_len]\n",
    "\n",
    "        ## We need to create weighted context vector\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        # attention weights = [batch size, 1, src_len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # encoder_outputs = [batch size, src len, enc hid dim * num_directions]\n",
    "        # perform weighted sum of encoder hidden states to get attention output\n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * num_directions]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * num_directions]\n",
    "\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        # weighted = [batch size, enc hid dim * num_directions]\n",
    "\n",
    "        ## concatenate the weighted context vector with the current decoder hidden state\n",
    "\n",
    "        conc = torch.cat((weighted, dec_hidden), dim = 1)\n",
    "\n",
    "        # conc = [batch size, enc hid dim * num_directions + dec hid dim]\n",
    "\n",
    "        ## Pass through the f_mid Linear layer to get S'\n",
    "\n",
    "        S_new = self.fc_mid(conc)\n",
    "\n",
    "        # S_new = [batch size, dec hid dim]\n",
    "\n",
    "        ## Then pass the f_out Linear layer to get predictions\n",
    "\n",
    "        prediction = self.fc_out(S_new)\n",
    "\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction, dec_hidden, dec_cell, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZWc56NpN32V"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # save the encoder-decoder attention weights\n",
    "        # all_attention_weights = [batch_size, trg len-1, src len ]\n",
    "        all_attention_weights = torch.zeros(trg.shape[1], trg.shape[0]-1, src.shape[0])\n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "          \n",
    "        hidden = torch.mean(encoder_outputs, dim=0)\n",
    "        cell = torch.cat((cell[0, :, :].unsqueeze(0), cell[1, :, :].unsqueeze(0)), dim = 2).squeeze(0)\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, cell, attention_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # all_attention_weights[t-1] = [src len, batch size]\n",
    "            all_attention_weights[:,t-1,:] = attention_weights.squeeze(1)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs,all_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "98SyJAyLN6jk",
    "outputId": "399d8098-3f04-4563-c285-b2ec1301606e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> token index:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter define\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "N_LAYERS = 1\n",
    "BI_DIRECTION = True\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 2 * ENC_HID_DIM #(you should figure out this hyper-parameter). \n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.8\n",
    "TEACH_FORCING_RATE = 0.7\n",
    "LEARNING_RT = 0.0012\n",
    "WEIGHT_DECAY = 0\n",
    "MAX_EPOCH = 10\n",
    "\n",
    "attn = Attention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT, BI_DIRECTION)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RT, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "print('<pad> token index: ',TRG_PAD_IDX)\n",
    "## we will ignore the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "ZmCZemJcOVdV",
    "outputId": "4015281e-2aad-4fde-b7f4-0b808fc0d3ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7891, 256)\n",
       "    (lstm): LSTM(256, 512, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention()\n",
       "    (embedding): Embedding(2320, 256)\n",
       "    (rnn): LSTM(256, 1024, dropout=0.8)\n",
       "    (fc_mid): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (fc_out): Linear(in_features=1024, out_features=2320, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    manual_seed = 77\n",
    "    torch.manual_seed(manual_seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed(manual_seed)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.05)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "colab_type": "code",
    "id": "HhTSZHHnOcpu",
    "outputId": "f32830f0-9725-4465-bf50-2fef5b1cb780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Tim0e: 6m 50s\n",
      "\t Train Loss: 1.238 | Train PPL:   3.447\n",
      "\t Val. Loss: 5.201 |  Val. PPL: 181.509\n",
      "\t Train BLEU:0.900 |  Val. BLEU:   0.259\n",
      "Epoch: 02 | Tim0e: 6m 54s\n",
      "\t Train Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 7.091 |  Val. PPL: 1201.357\n",
      "\t Train BLEU:0.962 |  Val. BLEU:   0.273\n",
      "Epoch: 03 | Tim0e: 6m 45s\n",
      "\t Train Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 7.705 |  Val. PPL: 2218.617\n",
      "\t Train BLEU:0.975 |  Val. BLEU:   0.277\n",
      "Epoch: 04 | Tim0e: 6m 45s\n",
      "\t Train Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 9.431 |  Val. PPL: 12463.416\n",
      "\t Train BLEU:0.984 |  Val. BLEU:   0.271\n",
      "Epoch: 05 | Tim0e: 6m 46s\n",
      "\t Train Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 11.020 |  Val. PPL: 61110.444\n",
      "\t Train BLEU:0.986 |  Val. BLEU:   0.266\n",
      "Epoch: 06 | Tim0e: 7m 0s\n",
      "\t Train Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 12.231 |  Val. PPL: 204988.977\n",
      "\t Train BLEU:0.989 |  Val. BLEU:   0.263\n",
      "Epoch: 07 | Tim0e: 6m 58s\n",
      "\t Train Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 13.226 |  Val. PPL: 554761.608\n",
      "\t Train BLEU:0.991 |  Val. BLEU:   0.270\n",
      "Epoch: 08 | Tim0e: 6m 53s\n",
      "\t Train Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 14.236 |  Val. PPL: 1522982.221\n",
      "\t Train BLEU:0.990 |  Val. BLEU:   0.263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-176821e41993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbleu_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f93168392a13>\u001b[0m in \u001b[0;36mtrain_attn\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mbleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/lab4/data/machine_translation/mt-train-pt2en.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d621c1b9e327>\u001b[0m in \u001b[0;36minference\u001b[0;34m(model, file_name, src_vocab, trg_vocab, attention, batch_size, max_trg_len)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtrg_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg_placeholder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m               \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#turn off teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m               \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_placeholder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#turn off teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-0e92e52604ef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m#insert input token embedding, previous hidden state and all encoder hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# all_attention_weights[t-1] = [src len, batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-d671d4727081>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, cell, encoder_outputs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# encoder_outputs = [batch size, src len, enc hid dim * num_directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# perform weighted sum of encoder hidden states to get attention output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#weighted = [batch size, 1, enc hid dim * num_directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "manual_seed = 77\n",
    "torch.manual_seed(manual_seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss,bleu_train = train_attn(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, bleu = evaluate_attn(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Create checkpoint at end of each epoch\n",
    "    state_dict_model = model.state_dict() \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': state_dict_model,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    torch.save(state, \"/content/drive/My Drive/Colab Notebooks/lab4/ckpt/mt_\"+str(epoch+1)+\".pt\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Tim0e: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(f'\\t Train BLEU:{bleu_train:.3f} |  Val. BLEU: {bleu:7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "YqCVRfHQP777",
    "outputId": "221b0dc4-6425-4855-d16f-e6bd298ff56a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do you have a ?', 'the bear did not laugh .', 'she has been here since an an hour .', 'try try the on on these jeans .', 'i hope to meet rest of .']\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "N_LAYERS = 1\n",
    "BI_DIRECTION = True\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 2 * ENC_HID_DIM #(you should figure out this hyper-parameter). \n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.8\n",
    "TEACH_FORCING_RATE = 0.6\n",
    "LEARNING_RT = 0.0011\n",
    "WEIGHT_DECAY = 0\n",
    "MAX_EPOCH = 10\n",
    "\n",
    "attn = Attention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT, BI_DIRECTION)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model_best = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "model_best.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/lab4/ckpt/mt_3.pt')['state_dict'])\n",
    "\n",
    "def translation_inference(token_id):\n",
    "  sent = []\n",
    "  for i in token_id:\n",
    "    if i == TRG.vocab.stoi[\"<eos>\"]:\n",
    "      break\n",
    "    else:\n",
    "      token = TRG.vocab.itos[i]\n",
    "      sent.append(token)\n",
    "  \n",
    "  return sent\n",
    "\n",
    "\n",
    "def generate_translations(model, eval_iter, trg_vocab, attention = True, max_trg_len = 64):\n",
    "  '''\n",
    "    Function for generating translation by model inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    eval_iter: iterator over the evaluation data\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optional), default = 64\n",
    "\n",
    "    Output:\n",
    "    List of translated sentences\n",
    "  '''\n",
    "  model.eval()\n",
    "  all_translation_word_ids = []\n",
    "  all_gold_sents = []\n",
    "  for batch in eval_iter:\n",
    "    src = batch.SRC\n",
    "    #src = [src len, batch size]\n",
    "    batch_size = src.shape[1]\n",
    "\n",
    "    # gold = batch.TRG\n",
    "\n",
    "    # for i in range(src.shape[1]):\n",
    "    #   gold_id = gold[1:, i].cpu().numpy()\n",
    "    #   gold_sent = translation_inference(gold_id)\n",
    "    #   all_gold_sents.append(gold_sent)\n",
    "\n",
    "\n",
    "\n",
    "    # create a placeholder for target language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "    trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "    trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "    trg_placeholder = trg_placeholder.long().to(device)\n",
    "    if attention == True:\n",
    "      output,_ = model(src, trg_placeholder) #turn off teacher forcing\n",
    "    else:\n",
    "      output = model(src, trg_placeholder) #turn off teacher forcing\n",
    "    # get translation results, we ignore first token <sos> in both translation and target sentences. \n",
    "    # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "    output_translate = output[1:]\n",
    "\n",
    "    # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "    prob, token_id = output_translate.data.topk(1)\n",
    "    translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "    # store gold target sentences to a list \n",
    "    all_translation_word_ids.append(translation_token_id)\n",
    "  \n",
    "  all_translation_text = []\n",
    "  for i in range(len(all_translation_word_ids)):\n",
    "    cur_translation_batch = all_translation_word_ids[i]\n",
    "    for j in range(cur_translation_batch.shape[1]):\n",
    "      trans_convered_strings = convert_itos(trg_vocab, cur_translation_batch[:,j])\n",
    "      all_translation_text.append(' '.join(trans_convered_strings)) # convert list of words to text\n",
    "  \n",
    "  return all_translation_text #, all_gold_sents\n",
    "\n",
    "def convert_itos(convert_vocab, token_ids):\n",
    "    list_string = []\n",
    "    for i in token_ids:\n",
    "        if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "            break\n",
    "        else:\n",
    "            token = convert_vocab.vocab.itos[i]\n",
    "            list_string.append(token)\n",
    "    return list_string\n",
    "\n",
    "translation = generate_translations(model, test_iter, TRG, attention = True, max_trg_len = 64)\n",
    "print(translation[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFFb39Lo3eHi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DVyaqHeVKzuy",
    "0n0Rz0kTRXWS"
   ],
   "machine_shape": "hm",
   "name": "lab4_MT_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
